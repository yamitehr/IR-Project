Computers in Human Behavior 57 (2016) 82e92
Contents lists available at ScienceDirect     
Computers in Human Behavior 
journal homepage: www.elsevier.com/locate/comphumbeh 
Full length article 
Improving social media measurement in surveys: Avoiding acquiescence bias in Facebook research   
Ozan Kuru*, Josh Pasek 
University of Michigan, Ann Arbor, United States 
article info 
Article history: 
Received 1 July 2015 
Received in revised form 
21 November 2015 
Accepted 2 December 2015 Available online 21 December 2015 
Keywords: 
Social media measurement Acquiescence 
Agreeedisagree scales 
Survey experiment 
Method biases 
Correlates of Facebook use 
1. Introduction 
abstract 
Social media measurement relies heavily on self-report survey research. Hence, known biases in how individuals answer survey questions can introduce systematic errors into the social media literature. In particular, many common social media measures are prone to acquiescence response bias, an error that occurs due to individuals' tendency to agree with agreeedisagree questions. The current study tests a series of techniques to both detect and overcome acquiescence bias in the context of Facebook mea surement. Controlling for individuals' tendency to agree with agreeedisagree questions, we find evi dence that acquiescence has inflated the reliabilities and factor loadings of many Facebook use scales, and has altered correlations both among Facebook use measures and between those measures and related covariates. Further, when the individual-level tendency to agree with questions is controlled, Facebook measures demonstrate greater criterion validity in their relations to items that do not use agree edisagree scales. Having identified the presence of acquiescent responding, we test three methods for mitigating this response bias: the use of balanced scales, item-specific questions, and statistical cor rectives. All three methods appear to reduce the bias introduced by acquiescence. Thus, the results provide comparative evidence on strategies to alleviate the consistent impact of an important method bias in social media measurement and thereby contribute to improving the validity of social media research at large. 
© 2015 Elsevier Ltd. All rights reserved. 
decry the use of AD Likert scales for social research (Fowler, 1995; 
Saris, Revilla, Krosnick, & Shaeffer, 2010). Yet, despite these calls, 
Acquiescence bias is a pervasive problem in survey research that could translate to social media measurement as well. When ques tions are presented with agreeedisagree (AD) or yes/no response options, some respondents select the “agree” (or “yes”) option disproportionately more frequently than the “disagree” (or “no”) option. Presumably, this tendency stems from the social norm to be agreeable (see Pasek & Krosnick, 2010). Acquiescence can introduce errors into data, as survey responses to acquiescence-prone mea sures conflate individuals' true attitudes and behaviors with agreeableness. At its most pernicious, this can lead survey re searchers astray, inducing correlations between similarly worded items that may be designed to tap unrelated constructs and hence resulting in systematic errors (Bagozzi, 1984; MacKenzie & Podsakoff, 2012). These concerns have led some researchers to 
* Corresponding author. Communication Studies, University of Michigan, 105 S. State St, 5370 North Quad, Ann Arbor, MI 48109, United States. E-mail address: okuru@umich.edu (O. Kuru). 
http://dx.doi.org/10.1016/j.chb.2015.12.008 
0747-5632/© 2015 Elsevier Ltd. All rights reserved. 
social media measurement through surveys, and especially the literature on Facebook, has extensively relied on survey techniques that are prone to acquiescence bias. 
Domains where AD scales are widely used might be particularly susceptible to inferential errors related to acquiescence; and research on the social network site Facebook appears to be one such area. With the explosive emergence of social media and the sub sequent proliferation of scholarship on Facebook from diverse fields (e.g. Wilson, Gosling, & Graham, 2012), a number of strategies have been introduced for gauging users' behaviors on the site (Kuru & Pasek, in press). The vast majority of studies of Facebook use, however, have been dominated by acquiescence-prone measures. Further, these measures tend to conflate agreement with greater use of the s/ocial network site. Acquiescence bias could therefore be an important confound in studies of Facebook, potentially hinder ing researchers' attempts at understanding and situating the social experience and consequences of site use. Since the Facebook liter ature is yet a young and emerging field, researchers may still have a 
O. Kuru, J. Pasek / Computers in Human Behavior 57 (2016) 82e92 83
chance to adjust course, tracking use of the site in ways that are less susceptible to bias. 
The current study assesses the extent to which acquiescence response bias may be influencing current studies of the correlates of Facebook use. To accomplish this, we look for the presence of method bias using an online survey experiment. Where method ological biases are observed, we test whether a variety of statistical correctives, balanced scales, and finally alternative question wording might mitigate these biases (cf. Saris et al., 2010). Struc tural equation modeling (SEM) and alternative question wording approaches are compared as potential ways to improve indexes of Facebook use as well as in their ability to predict a variety of theoretically related constructs. 
2. Literature review 
2.1. Acquiescence bias 
Survey methodologists have long noted the tendency of re spondents to agree when confronted with AD questions, regardless of their content (Billiet & McClendon, 2000; Jackson, 1959; Welkenhuysen-Gybels, Billiet, & Cambre, 2003   ). Nonetheless, Lik ert scales using these response options remain prevalent throughout social science research. AD questions are simple to format and easy to generate, which may explain their prevalence (Saris et al., 2010), but this simplicity comes at a cost. Acquiescent responses can lead scholars to incorrect conclusions because they confound the attitudes and behaviors studied with the general tendency to agree. And acquiescence can have a large impact; multiple studies suggest that between 10% and 20% of respondents engage in this behavior (Krosnick & Fabrigar, 2001; Saris et al., 2010; Schuman & Presser, 1981). 
Studies of acquiescence response bias indicate that the tendency to acquiesce varies across individuals (MacKenzie & Podsakoff, 2012). Three theories have been proposed for why some people behave in this manner. First, considerable evidence shows that respondents treat surveys as conversational (Brown & Levinson, 1987; Pasek & Krosnick, 2010). Following conversational conven tions, respondents may agree with survey prompts because “in everyday conversations people want to be agreeable and want to be agreed with” (Brown & Levinson, 1987; Leech, 1983). A second possible cause of acquiescence stems from the perceived authority of the interviewer. Reacting to this role, respondents might defer to the interviewer, acquiescing to his or her question by responding in agreement (Carr, 1971; Lenski & Leggett, 1960). Finally, it is possible that acquiescence is a product of survey satisficing (Krosnick, 1991; Simon, 1957). Instead of pondering the question and contemplating all the information available to provide the best possible answer, respondents might satisfice by choosing an answer that seems good enough or appropriate. Agreeing with the prompt appears to be one mechanism for choosing just such an answer (Krosnick, 1999, 1991). 
Apart from these theoretical mechanisms, individual differences in cognitive abilities and education (Krosnick, 1999; Krosnick & Alwin, 1987; Schuman & Presser, 1981), psychological tendencies toward impulsiveness (Couch & Keniston, 1960; Messick, 1991), and agreeableness (Costa & MacCrae, 1992; Knowles & Condon, 1999) can alter the level of acquiescence bias (MacKenzie & Podsakoff, 2012). This suggests that acquiescence can be regarded as an in dividual-level trait. 
2.2. Addressing acquiescence 
Three basic strategies have been proposed for mitigating the influence of acquiescence on research conclusions. The first in volves the use of “balanced” scales, in which respondents are asked 
whether they agree with conflicting statements about a concept (Nunnally, 1978; Schriesheim & Hill, 1981). Although these scales reduce the correlation between acquiescence and the construct of interest, they arbitrarily place individuals who always acquiesce at the midpoint of the scale, which may or may not be reasonable (Billiet & McClendon, 2000; Pasek & Krosnick, 2010). A second strategy involves statistically correcting for acquiescence. Tech niques such as factor analyses (Billiet & McClendon, 2000; Cheung & Rensvold, 2000; Podsakoff, MacKenzie, Lee, & Podsakoff, 2003; Welkenhuysen-Gybels et al., 2003; Williams, Gavin, & Williams, 1996) and ipsatization (Chan, 2003; Fischer, 2004; Greenleaf, 1992) can be employed to disentangle acquiescence from the construct of interest. Finally, researchers can ask questions that are not susceptible to acquiescence response bias to start with. 
In particular, replacing AD scales with direct queries about the concepts of interest e so-called “item-specific” (IS) questions e can result in measures with potentially greater reliability and validity (Ross, Steward, & Sinacore, 1995; Saris et al., 2010; Schuman & Presser, 1981). Instead of assessing whether respondents agree or disagree with a prompt, IS-questions solicit the extremity of an attitude or the extent of a behavior using response options that mirror that of the question. For example, instead of asking people how much they agree or disagree with the prompt, “I like cookies,” researchers could instead ask respondents, “How much do you like cookies?” with response options ranging from “Not at all” to “A great deal.” 
2.3. Social media measurement: Facebook scales and acquiescence 
With the proliferation of social network sites, researchers have scrambled to measure how people are using the sites as well as the consequences of site use. As Facebook experience became a part of social, political, and economic life, researchers from various disci plines incorporated measures of Facebook use in their studies (see Wilson et al., 2012 for a review). Faced with the need to rapidly develop and deploy scales, it is unsurprising that researchers generously borrowed measures and scales from a few early papers that established them (see Kuru & Pasek, in press). Unfortunately, validated scales are scarce, and a majority of the scales are composed of Likert-style AD questions (examples below). Hence, if acquiescence response bias is a problem for AD scales measuring Facebook, it could represent a limitation for much of the literature. 
There are a variety of reasons to worry that acquiescence response bias could be altering our understanding of social network use and particularly Facebook use. First, as noted above, AD Likert style questions compromise the bulk of measures for many commonly used Facebook scales. A recent systematic review of Facebook studies published since 2007 until the end of 2014 reveals that 20 of the 47 studies used agreeedisagree format in measuring Facebook use (Kuru & Pasek, in press). Moreover, many of the widely adopted and used scales such as the Facebook Intensity Scale (Ellison, Steinfield, & Lampe, 2007) also rely on the acquiescence-prone Likert format. Second, these scales have been analyzed without any attempt to assess the extent of acquiescence, meaning that the potential for bias is unknown. Third, the previous research on response styles had found that response biases can alter the correlations between constructs (Baumgartner & Steenkamp, 2001). And finally, agreement with AD measures in Facebook use scales almost always corresponds with greater use of the social network site (i.e. the scales are not balanced), meaning that serial acquiescence could introduce a clear directional bias into correlations. 
Acquiescence bias could present a cascade of problems for the literature. First, confounding respondents' tendency to agree with Facebook use could induce spuriously inflated correlations 
84 O. Kuru, J. Pasek / Computers in Human Behavior 57 (2016) 82e92
amongst Facebook scales. Second, the association between Face book use and other social variables of interest e such as social capital, political participation, relationship satisfaction, academic success, or psychological well-being e would conflate substantive findings with correlates of the tendency to acquiesce. And third, aside from these within-study effects, systematic errors might then accumulate across studies biasing our understanding of social media use and its correlates. 
The potential for acquiescence bias to undermine our under standing of Facebook use and its correlates can be seen in studies linking Facebook with social capital. In most studies, correlations between the frequency or intensity of Facebook use and social capital have been positive (e.g. Ellison et al., 2007; Kwon, D'Angelo, & McLeod, 2013; Lee, Kim, & Ahn, 2014; Park, Han, & Kaid, 2012). But all nine studies making this comparison used the predomi nantly AD Facebook Intensity Scale.1 For these studies, acquiescent responding might have increased the strength of the correlations that were observed. Given the consistent use of AD scales across these studies, we do not know how robust these relations are to controls for acquiescent responding. 
2.4. The current study 
The current study presents an online survey experiment that explores the scope and implications of acquiescence response bias in measures of the social network site Facebook and compares possible remedies. We test for the presence of a method bias in responses to agreeedisagree (AD) questions for four commonly employed indexes measuring Facebook use. We also incorporated two important covariates that have been widely studied in the literature for analytical and theoretical comparisons, a balanced scale measuring self-esteem (Mehdizadeh, 2010; Rosenberg, 1965; Steinfield, Ellison, & Lampe, 2008) and a non-balanced scale of bridging social capital (Ellison, Steinfield, & Lampe, 2007; Williams, 2006). 
In this survey experiment we examined the potential conse quences of dependence on the AD response format in four scales measuring aspects of Facebook use. The scales were chosen because they have come to dominate the early literature and are considered to capture important dimensions of engagement with Facebook: 
  The Facebook Intensity Scale (Ellison et al., 2007) has been used in numerous studies and consists of two parts: two items that measure the time spent on Facebook and number of friends and a six question Likert battery with AD options. The measure as sesses the strength of a user's social network and attitudes and behaviors about usage. In this study, the six item Likert battery is used whereas time spent and number of friends are excluded from the factor analysis of this scale as they are not attitude items. 
  The Trust Scale (Jarvenpaa, Tractinsky, & Saarinen, 1999; Krasnova, Spiekermann, Koroleva, & Hildebrand, 2010; McKnight, Choudhury, & Kacmar, 2002)2 measures the extent to which Facebook users trust one-another as well as their at titudes toward the site with six items that have AD options. 
1 The numbers presented here come from a recent book chapter by the authors (Kuru & Pasek, in press). 
2 Some citation years that date back before the launch of Facebook should not be confusing and comes in dialogue with our concerns regarding measures of social media activity/usage. Most of early Facebook scales have been partially adapted from previous measures of Internet or other online activity measurement, thus contributing to the widespread use of highly problematic AgreeeDisagree response options. 
  The Facebook Self-Disclosure Scale (Krasnova, Kolesnikova, & Guenther, 2009) measures the level of information that users share about themselves on Facebook, this six item scale with AD options relates theoretically to the potential of online privacy issues. 
  The Facebook Intrusion Scale (Elphinston & Noller, 2011) mea sures the excessive use of and dependency on Facebook with eight items that have AD options. 
The measures represent only a small subset of the scales that have been employed in the literature, but we chose them for their prominence in the literature as well with the fact that they tap distinct, yet related dimensions and domains of use (Kuru & Pasek, in press; Wilson et al., 2012). These measures have allowed re searchers to make enormous progress in exploring the correlates of Facebook use, relating use to phenomena as diverse as education (Junco, 2012; Wise, Skues, & Williams, 2011), self-esteem (e.g. Ellison et al., 2007; Mehdizadeh, 2010), personal relationships (e.g. Elphinston & Noller, 2011), social capital (e.g. Ellison, Steinfield, & Lampe, 2011; Ellison, Vitak, Gray, & Lampe, 2014), and political participation (Bode, 2012; Vitak et al., 2011). 
2.5. Hypotheses 
Based on longstanding evidence of acquiescence to these types of questions, we hypothesize that a strong method factor will predict agreement to questions across all AD scales (H1). 
Because the systematic error of acquiescence theoretically in flates correlations between directionally similar measures, con trolling for the general tendency to agree using a method factor should reduce correlations among measures of Facebook use (H2a). Further, by the same logic, controlling for a method factor is ex pected to decrease correlations linking Facebook measures with a similarly unbalanced covariate, bridging social capital (H2b). In contrast, correlations should not be altered between Facebook measures and a balanced covariate, self-esteem (H2c). 
Next, we compare the AD measures as well as statistical controls to a series of criterion variables to assess the validity of numerous strategies for correcting for acquiescence bias. Here, we expect that correlations with major social network site indicators (such as number of friends, time spent, frequency of use, and account accessibility levels) would be lower in AD scales than for AD scales where a method factor was controlled (H3). This would in turn imply that mitigating acquiescence bias increases validity. 
Next, we test whether the use of item-specific (IS) versions of these measures might circumvent acquiescence. IS measures are expected to be free of acquiescence bias because agreement is not an option. Hence we hypothesize that the influence of controlling for a method factor will be demonstrably smaller when using IS measures instead of AD measures (H4) and that the IS measures will also demonstrate greater criterion validity than the AD mea sures (H5). 
3. Methods 
3.1. Sample 
This survey experiment was conducted among individuals from two non-probability sample sources in 2013. The first sample consisted of 164 undergraduate students from a departmental research pool at a large Midwestern university. Individuals in the student sample received course credit for completing a specified number of studies or could complete an alternative assignment. The second sample includes results from 605 participants recruited through Amazon's Mechanical Turk (“Turkers”). We posted the 
O. Kuru, J. Pasek / Computers in Human Behavior 57 (2016) 82e92 85
current study as an assignment on Mechanical Turk, which in dividuals could complete by following a link to a Qualtrics survey. Individuals were compensated $.75 if they posted a code that was provided to them upon completion of the survey. Individuals were limited to completing a single survey.3 All respondents, regardless of sample source, were randomly assigned to one of the experi mental conditions. They were either presented with measures us ing the original AD scales or the equivalent IS versions.4 Online Appendix A shows descriptive statistics as well as results check ing the equivalence of the different samples (see also Table B1 in Online Appendix B). 
3.2. Measures and design 
3.2.1. AD measures 
Four scales of Facebook use were analyzed: The Facebook In tensity Scale (FBI) (6 items, unbalanced), Trust on Facebook (6 items, unbalanced), Facebook Self-Disclosure (6 items, unbal anced), and Facebook Intrusion (8 items, unbalanced). Additionally the Bridging Social Capital Scale (8 items, unbalanced; Ellison et al., 2007)5 and Rosenberg Self-Esteem Scale (10 items, balanced; Rosenberg, 1965) were included in the analysis as Facebook use correlates. AD versions of these scales were comprised of five- and seven-point questions. Responses were recoded to range from 0 to 1, where 0 represented the lowest possible score (“strongly disagree”) and 1 represented the highest value (“strongly agree”), with all other responses assigned to intermediate values (e.g. “agree” ¼ .75). Full question wordings and response options appear in Online Appendix C. 
3.2.2. IS measures 
Based on Saris et al.'s (2010) study, IS versions of each question were created. For example, the AD version of attitude item 4 in the Facebook Intensity scale, “I feel out of touch when I haven't logged onto Facebook for a day,” with the response options “Strongly Disagree” (coded: 0), “Disagree” (.25), “Neither Agree nor Disagree” (.5),” Agree” (.75), and “Strongly Agree” (1) was replaced with “Do you feel out of touch if you do not log into Facebook for a day?” with the response options “Never feel out of touch” (0), “Rarely feel out 
3 Mechanical Turk is widely used in social science research and provides more representative samples than college undergraduate participants (Berinsky, Huber, & Lenz, 2012; Buhrmester, Kwang, & Gosling, 2011). Each Amazon worker could take the survey only once and instructions were given for completing the survey within a reasonable time. Although a significant difference in survey completion time is observed between the two samples e mean completion times for MTurk (13.1) was only about 1 min shorter than the mean of the student sample (14.4) e this small difference should not raise data quality concerns. 
4 In practice, there were two unique random assignments that were cross-cutting in the current study. Individuals were randomly assigned either the AD or IS version of the Facebook measures and were also randomly assigned either the AD or IS version of the outcome measures (Self-Esteem and Social Capital). Estimates of factors for all measures were generated using all individuals who answered each version of the scales. Estimates for correlations across factors were generated using pairwise complete observations. When data were combined in AMOS, we used maximum likelihood imputation to estimate all relations. This procedure provides more accurate estimates so long as missing data are missing at random, which should be ensured given the random assignment of individuals to experimental conditions. Nonetheless, to ensure that these crossover results were not yielding different estimates, we replicated all analyses with AD-only and IS-only sub samples. Differences were uniformly minor and would not alter any of our conclusions. 
5 Ellison, Steinfeld and Lampe (2007) used 9 items to measure bridging social capital; one of these items was not included in the current study because the wording was deemed awkward. Although this makes the scale a little different from than what had been used in that study, our major focus in this study is to compare a set of scales across diverse acquiescence strategies. The theoretical re sults pertain to comparative statistics regarding acquiescence. 
of touch” (.25), “Sometimes feel out of touch” (.5), “Most of the time feel out of touch” (.75), and “Always feel out of touch” (1). The goal of this procedure was to remain as loyal as possible to the original question meaning in an IS format. Full question wordings appear in Online Appendix C. 
3.2.3. Covariates 
Aside from the common scale questions about Facebook use, four measures of Facebook network activity were considered as covariates to serve as criterion validity checks. These included number of Facebook friends, time spent on Facebook, frequency of log-in, and the visibility/accessibility level of respondents' accounts.6 
3.2.4. Equivalence of questions across conditions 
Participants were experimentally assigned to receive either AD or IS question wordings and results from both types were compared. By changing response options while keeping the ques tion stems as similar as possible, this design does as little as possible to alter the way that respondents should interpret the questions; responses should therefore remain true to the latent constructs proposed.7 Results checking the equivalence of experi mental conditions are shown in Online Appendix A. 
3.3. Analytical procedure 
To assess the presence of a method factor in the AD measures of Facebook use (H1), we compared two confirmatory factor analyses (CFAs) using structural equation modeling in the SPSS Amos soft ware (see Online Appendix D for full CFA models in AMOS). The first model was a traditional CFA model, where indicators of each of the four Facebook and two covariate scales were parameterized as a function of the relevant underlying latent construct and some unique indicator-level error (see Fig. 1A for a simplified version). The second model was a method-factored model; in this model, indicators of each of the separate scales were treated as a product of the relevant underlying latent construct, a consistent methodo logical bias across all similarly-worded measures, and unique indicator-level error (see Fig. 1C). A latent factor representing the methodological bias or “method factor” was added for the method factor model (and shown separately in Fig. 1B) and was constrained to load consistently across all measures that were similarly worded, it is thus a measure of the similarity between measures that is induced by the way the measures are constructed (Billiet & McClendon, 2000).8 If the method factor accounts for shared 
6 Number of friends was an open-ended question. Time-spent had 7 response options with 30 min of intervals from “0e30 min daily” to “more than 3 h”. Fre quency of logging-in had 6 options ranging from “1 times” to “more than 6 times”. Those who stated they were always logged-in were excluded from this variable as this option could not be enumerated along the frequency variable in a theoretically meaningful way. Accessibility level of the account, asked as “What is the general accessibility of your account?” had 4 options: “only you”, “your friends only”, “friends of your friends as well”, and “everybody/open to the public”. These mea sures are important structural properties of Facebook activity/use measurement and reported in nearly all studies. In this study they additionally (1) serve as cri terion validity check for the alternative IS measures we develop and (2) their cor relations with AD measures are compared across experimental conditions for inferring acquiescence bias effects. Please see Online Appendix B for further details. 
7 We adopted this approach in the current study to minimize differences be tween the two sets of measures. An ideal design approach for IS measures, how ever, would involve changing the question wording to more closely match the response options. 
8 This assumption is appropriate as acquiescence appears to be an individual level trait (Weijters, Geuens, & Schillewaert, 2010). 
86 O. Kuru, J. Pasek / Computers in Human Behavior 57 (2016) 82e92                        











Fig. 1. Simplified Model Representations in the SEM. Large circles indicate latent variables, small circles represent error terms, and rectangles represent observed variables. Bolded lines in Part C have forced factor loadings of 1 (or  1 for reversed items) whereas coefficients for un-bolded lines were allowed to vary. 
variance between the items (H1), its inclusion should result in improved goodness of fit compared with the traditional CFA model.9 Significant and sizable loadings on this method factor would provide further evidence for H1. 
Evidence that methodological biases were inflating correlations between unbalanced measures can also be evaluated by comparing a method factor model with a traditional CFA model (H2). Here we examined the responsiveness of correlations between latent vari ables to the presence of the method factor. 
Finally, we examined the criterion validity of the AD measures as well as method-factored versions of those measures. All four sets of measures were compared with the four criterion variables measuring social network site use. Stronger correlations with each of these measures would be indicative of better measurement (H3). Hence if the method factor model produced higher criterion cor relations it would imply that methodological bias was present. 
To assess the potential use of IS alternatives to the AD measures, IS items were subjected to all the same analyses as the AD versions. We examined whether IS questions were subject to similar meth odological biases by comparing a standard CFA with the IS mea sures with a method factored alternative for model fit, factor loadings, inter-correlations, and reliability (H4). We also compared the IS measures to the AD measures to see what model provided the most valid results when related to criterion variables (H5). Tests of all hypotheses are summarized in Table 1. 
In order to assess the comparative performance of each of the different models we used, we needed to determine whether factor loadings and correlations were systematically larger or smaller in some methods than in others. This determination was complicated, however, by the fact that the factor loadings and correlations of interest within each of the methods varied drastically in size, and 
9 Model fit indexes tell us how much our theoretical measurement models accord with the data. Model fits were assessed using four fit indices (Bentler, 1990). First we use chi-square, which assesses whether the variance accounted for differs significantly from a saturated model. Nonsignificant p values indicate that the model captures all relevant variance. Notably, this is rarely true with samples of N > 100. The second fit index we employ is the root-mean square error of approximation test (RMSEA), which tests the parsimony of the model. In an ideal model, RMSEA should have a value under .05 (Kline, 2005). Third, we use the comparative fit index (CFI), which produces estimates similar to Chi-squared, but is less sensitive to sample size; the CFI value should ideally be larger than .95 for a good fit (Bentler & Bonett, 1980; Hu & Bentler, 1999). In addition to these, we also assess AIC, which can be used to compare nested models. Lower AIC scores indicate better fitting models within the same family. 
our hypotheses were about the strength of relations rather than their absolute direction. For this reason, a simple t-test comparison of the factor loadings or correlations would not be able to detect a result even if systematic differences existed. To circumvent this problem, we used a linear mixed model for significance tests.10 
4. Results 
4.1. Method bias in agreeedisagree measures 
Method biases in AD measures were apparent when comparing the model fits of the traditional and method factor CFA models. Both traditional and method factor models provided an acceptable fit for the AD data (Table 2, columns 1 and 2). Both failed the chi square test (ps < .001) but passed the root-mean square error of approximation test (RMSEA values < .08 and mostly <.05) and showed CFI values near traditional cut-offs. These values are indicative of models using large samples that fit the data well, but could be more parsimonious (cf. Hu & Bentler, 1999).11 
Goodness of fit statistics were preferable for all measures using the method factor model as compared to the traditional model. For example, chi-square for the traditional AD model was 2555 whereas the method factor solution chi-square value was 2435 (p < .001 difference in a nested model comparison). Improvements were also apparent for RMSEA, which dropped from .049 to .048, and CFI, which increased from .86 to .87. Although these differences were small, they were indicative of a method factor that is related to a small but systematic error that skews the data. The constrained 
10 The general formula we tested was Coefmi ~ b model þ RE(Comparison). In this model, each individual coefficient we are estimating is regarded as a function of the model that is being used (e.g. AD vs. AD with method factor) and includes a random effect for the estimated value of the comparison we are approximating (e.g. the factor loading of a particular item on the FBI scale). The b coefficient for the model captures the average difference between coefficients in the models being compared while simultaneously attempting to control for the different expected coefficients of the comparisons that are being estimated. That is, the random effects allow us to account for the vast coefficient differences that are expected as a function of widely varying comparisons and thus to exclude error related to those differences from our comparison of models. 
11 This less-than-perfect fit is perhaps unsurprising as we might expect some kind of second order factor that accounts for commonalities between Facebook mea sures. The presence of such a factor is not important for the current purposes as the central interest is in comparative fit between models. There is no formal test for differences between these coefficients. 
O. Kuru, J. Pasek / Computers in Human Behavior 57 (2016) 82e92 87
Table 1 
Analytical tools in the confirmatory factor analysis data. 
What we compare Evidence it provides 
Model fits Model fits allow us to compare the overarching effect of systematic error across the models 
Factor loadings A decrease in factor loadings for method factor models or alternative question wording (item-specific) compared to the acquiescence-prone measures is indicative of acquiescence 
Factor correlations Acquiescent responding should inflate correlations between unbalanced factors that use acquiescence-prone measures 
Criterion validity correlations 
Table 2 
Measures with less bias should correlate more strongly with exogenous correlates that do not use acquiescence-prone scales 
Average loadings and goodness of fit statistics of confirmatory factor analysis (CFA) models. 
Average standardized loadings Agreeedisagree (AD) CFA AD method factor model Itemespecific (IS) CFA IS method factor model 
FBI .76 .72 .71 .69 Trust .81 .76 .76 .73 Disclosure .83 .80 .71 .68 Intrusion .68 .64 .63 .60 Social Capital .79 .74 .74 .71 Self Esteem .70 .70 .71 .71 Method Factor e .21 e .14 Average* .77 .73 .71 .68 Goodness of fit 
Chi-square 2555 2435 2144 2049 df** (887) (886) (887) (886) CFI .86 .87 .87 .88 RMSEA .049 .048 .043 .041 AIC 2849 2731 2438 2345 
Notes. Loadings presented are the statistical mean of the absolute values of all indicators for the latent constructs shown. All standardized loadings are significant at the p < .001 level. Average* indicates the average of the average factor loadings for FBI, Trust, Disclosure, Intrusion and Social Capital items (all unbalanced scales). The balanced self-esteem measure was not included in this average. **Degrees of freedom represent the difference between the number of distinct sample moments and the number of distinct parameters that are estimated. 
Table 3 
Random effects regressions assessing differences in coefficients between models. 
Factor loadings (N ¼ 34) FB correlations (N ¼ 10) SNS correlations (N ¼ 20) 
AD vs Method AD Method b  .04***  .06*** .01** s.e. .003 .01 .005 
AD vs IS Method b  .06***  .001 .09*** s.e. .01 .02 .02 
Method AD vs IS Method b -.01 .06* .08*** s.e. .01 .02 .01 
IS vs Method IS Method b  .03***  .05*** .0001 s.e. .002 .008 .002 
Notes. Each coefficient shown presents the result from a separate linear mixed model. Each model treats the loadings/correlations derived using the various methods as a function of (1) a random effect indexing what correlation is being predicted and (2) a dummy variable indicating the method used to predict that correlation. For the sake of parsimony, only the dummy variables are presented. Self-esteem items have been excluded as theoretically they are expected and found to be invariant across the models. *** p<.001, ** p<.01, * p<.05, two-tailed. 
model (i.e. with a method factor) thus represented a significant improvement over the unconstrained model and an improvement for all measures of model fit. Hence, these results were in line with the hypothesized presence of acquiescence bias. 
Comparing factor loadings across the CFA models provides additional evidence that methodological bias was present when using the AD questions. Table 2 presents the average standardized factor loadings for each of the Facebook and covariate scales (loadings for all 44 individual items can be found in Online Appendix E). Differences in factor loadings between the tradi tional and method factor CFAs revealed a consistent pattern whereby loadings were markedly higher when the method factor was not included (Table 2, columns 1 and 2). For example, the average FBI loading dropped from .76 to .72. Similar drops were observed in all other unbalanced scales, including the non Facebook measure of social capital; a linear mixed model revealed that standardized factor loadings for the method factor model were significantly different from the traditional CFA model 
(b ¼  .04, s.e. ¼ .003, p < .001, Table 3). These results indicate that standardized loadings for the AD model were inflated by an average of approximately .04 (Table 3).12 Notably, coefficients for the balanced self-esteem measure did not diminish when the method factor was introduced. Since balanced scales are designed to miti gate errors due to acquiescence bias, this result further confirms that drops in other factor loadings were likely a function of serial agreement. 
Loadings for the method factor itself further illustrate the extent of bias in the AD measures. The average standardized loading of AD measures on the method factor was .21. This suggests that methodological bias accounted for at least one-fifth of the variance in the indicators (Table 2, column 2; individual loadings 
12 As an alternative approach we also compare Chronbach's alphas for non ipsatized and ipsatized measures to assess the maximum possible extent of this bias in Online Appendix H. 
88 O. Kuru, J. Pasek / Computers in Human Behavior 57 (2016) 82e92
are presented in Online Appendix F). Loadings on the method factor thus reinforce evidence of the presence and problematic conse quences (i.e. factor loading inflation) attributable to acquiescence bias. 
4.2. Factor correlations for AD measures 
Further evidence of the prevalence and impacts of methodo logical bias can be assessed by examining correlations amongst the latent factors measuring Facebook use and other covariates. Cor relations between all six scales in the AD models with and without a method factor are reported in the upper half of Table 4. Comparing correlations across these two models, we can identify a pattern of consistent decreases in the correlations observed when the method factor was introduced. For example the correlation between FBI and Trust variables decreased from .45 to .36 and the correlation between FBI and Disclosure measures dropped from .57 to .51 when the method factor was included. Amongst the mea sures of Facebook use, this decline in correlations was universal. On average, correlations between measures in the method factor model were only 83% as large as corresponding correlations in the traditional model. Further a linear mixed model revealed that the differences in correlations between models were statistically sig nificant (b ¼  .06, s.e. ¼ .01, p < .001, Table 3). This suggests that correlations across measures were inflated by methodological bias. 
Results comparing Facebook scales with the two covariate scales across models provided additional evidence that this bias was indeed acquiescence. Correlations between the Facebook measures and the unbalanced scale measuring social capital behaved in a manner similar to those amongst the Facebook scales; weaker correlations were observed when a method factor was controlled (Table 4, row 5). This indicates that decreased correlations were not 
Table 4 
Correlations between latent variables across models. 
due to some unique feature of Facebook use. For the balanced co variate scale measuring self-esteem, this pattern was not evident. Instead, correlations linking self-esteem with other scales remained consistent when the method factor was controlled (Table 4, row 6). The lack of a similar pattern for the balanced scale again confirmed that methodological biases were related to the direction of the questions. 
4.3. Correlations between AD measures and criterion variables 
Comparing the traditional and method factor AD models with a series of criterion variables yielded evidence that latent factors in the method factor model were more valid than those in the tradi tional model. Correlations between criterion variables and the Facebook scales were found to be either equivalent or slightly stronger when the method factor was controlled (compare corre lation coefficients for columns 1 and 2 of Table 5). Across all 20 correlations with unbalanced measures, the strength of relations using the method factor model was significantly stronger than that of the traditional AD model (b ¼ .01, s.e. ¼ .005, p ¼ .01, Table 3). These results provide evidence that methodological bias was reducing the validity of the Facebook scales. 
4.4. The item-specific alternative 
When IS measures were used in place of the AD measures, the influence of controlling for a method factor dropped precipitously. In contrast with overstated correlations between scales and diminished validity revealed by the method factor for the AD measures, the use of a method factor with the IS measures revealed smaller inflations in the reliability of these measures and no apparent influence on their validity. Factor loadings for the IS 
Table 5 
Correlations with social network criterion variables. 
O. Kuru, J. Pasek / Computers in Human Behavior 57 (2016) 82e92 89Agreeedisagree (AD) AD with method factor Itemespecific (IS) IS with method factor 
Number of Friends FBI .14** .16** .24** .24** Trust -.02 -.01 -.06  .07 Self-disclosure .11* .13* .16y .16y Intrusion .17** .19*** .26* .26* Social capital .19*** .22*** .22* .22* Self-esteem -.02 -.02 .03 .03 
Time Spent on Facebook FBI .53*** .54*** .66*** .66*** Trust .17* .17* .31*** .29** Self-disclosure .30*** .32*** .45*** .44*** Intrusion .38*** .40*** .61*** .60*** Social capital .19*** .20*** .18* .16y Self-esteem .01 .01 -.02  .02 
Frequency of Use FBI .60*** .62*** .77*** .78*** Trust .11y .11y .22* .22* Self-disclosure .27*** .28*** .50*** .51*** Intrusion .38*** .40*** .58*** .60*** Social capital .28*** .28*** .29** .29* Self-esteem .04 .04 .06 .05 
Accessibility of Facebook FBI .15** .15** .22** .22* Trust .11* .11y .16* .16y Self-disclosure .16*** .16** .24* .24* Intrusion .15** .15** .11 .11 
Social capital .11* .11* .11 .11 self-esteem .11* .11* .05 .06 Overall Average .226 .235 .317 .317 
Note. Averages were computed as the average of the absolute values for all correlations in each model. *** p<.001, ** p<.01, * p<.05, y p<.10, two-tailed. 
measures also diminished when a method factor was controlled (b ¼  .03, s.e. ¼ .002, p < .001; Table 3), providing evidence that some methodological consistencies between items remained. Notably, this bias was slightly smaller than the bias observed for the AD measures and the loadings on the method factor were less substantive (averaging .14 instead of .21; Table 2). Model fits in Table 2 (columns 3 and 4) reflected this trend. Slight improvements were apparent when going from the IS model to the IS model with method factor. A nested model comparison showed that differences between the fits for the simple model (c2(887) ¼ 2144) and method factor model (c2(886) ¼ 2049) were statistically significant (df ¼ 1, p < .001), indicating that IS measures may not be immune to methodological effects. Further, controlling for methodological similarity resulted in a decrease in correlations between similarly measured constructs (b ¼  .05, s.e. ¼ .008, p < .001; Table 3). Hence, it appears that some methodological consistency exists across the IS measures as well, though this was smaller than for the AD measures. 
Despite evidence of a method factor for the IS measures, con trolling for this method factor did not yield the same improvements in criterion validity. In contrast with the AD measures, there was no change in the average correlations linking Facebook scales with social network criterion variables when the method factor was controlled (b ¼ .00, s.e. ¼ .003, p ¼ .99). Hence, methodological bias for IS measures, though apparently present, did not appear to have been as problematic as those of agreeedisagree scales. 
4.5. Comparing AD and IS models 
Because AD and IS models were conducted on different sub samples (experimental conditions), model fits are not directly comparable between these two conditions. These differences thus could not be directly tested for significance in a nested model. To circumvent this challenge, a Monte Carlo simulation was used to test whether going from AD to IS improved model fits significantly. This nonparametric simulation indicated that the difference be tween AD and IS chi-squares was not significant, however it did approach significance (p < .07). For details of this procedure please refer to Online Appendix G. Additionally the AIC model fit index 
was higher for the AD (2849) model than the IS (2438) model, which suggests better overall fit for the IS models. However these significance tests of the model differences are not conclusive. 
There is some suggestive evidence that the IS model fits its data better than the AD model. In particular, factor loadings in the IS model e although lower than in the AD model (b ¼  .06, s.e. ¼ .01, p < .001; Table 3) e were not distinguishable from the method factor AD model (b ¼  .01, s.e. ¼ .01, p ¼ .28; Table 3). These results are in line with H4, that the IS measures should perform more consistently than the AD measures. 
Similarly, the IS measures strongly outperformed the AD mea sures in terms of criterion validity. Correlations linking the IS measures with criterion variables were, on average, .09 stronger than identical correlations for the AD measures (s.e. ¼ .02, p < .001; Table 3). For example, relations between FBI (excluding the number of friends item as asked in the traditional version of this scale) and respondents' reported number of friends was only .14 with the AD measures and was .24 with item-specific measures. In total, 18 out of 20 criterion variable correlations were stronger with the IS measures, indicating a consistent measurement advantage. This improvement was also apparent when comparing the method factor model for the AD measures with the IS measures as IS measures had stronger correlations with criterion variables (b ¼ .08, s.e. ¼ .01, p < .001; Table 3). Hence, criterion validity tests presented some evidence that the IS measures were less sensitive to methodological bias than the AD measures. 
5. Discussion 
5.1. Summary of results and implications 
This study presents a first assessment of the potential that acquiescence bias may be important in scales measuring Facebook use. All hypotheses were fully or mostly supported. Overall, results indicated that acquiescence-prone questions artificially inflated the reliability of measures while simultaneously understating the val idity and predictive capacity of these scales. Collectively, the results suggest that acquiescence is an important problem in Facebook measurement with the potential to consistently distort research 
90 O. Kuru, J. Pasek / Computers in Human Behavior 57 (2016) 82e92
conclusions about the social network site and its correlates. We also found that, aside from statistical correctives and balanced scales, item-specific (IS) response scales may provide a low-effort strategy for avoiding potential confounds. 
In particular, the current results indicate that the agreeedis agree (AD) questions that dominate contemporary scholarship should not be preferred. These questions appear to result in inflated factor loadings for latent constructs, artificially high reliability scores, and induced correlations between Facebook use and other constructs that use similar measurement scales. Our results largely replicate evidence from other assessments of agreeedisagree questions that reveal similar biases (Saris et al., 2010). 
In addition to finding indications of a pervasive bias, the current study compares evidence for three major approaches for mitigating the errors attributable to acquiescence. These include the use of balanced scales (Cloud and Vaughan, 1970), method factors in structural equation models (MacKenzie & Podsakoff, 2012), and the use of an alternative set of response options (IS scales) that have been theorized to avoid methodological biases (Fowler, 1995; Saris et al., 2010). Evidence for the validity of all three approaches was apparent. As expected, the balanced scale used in the study was not influenced by the introduction of a method factor, suggesting that this method indeed avoided systematic bias. Models controlling for a method factor also behaved as expected with regard to factor loadings and correlations, indicating that this strategy was also effective at identifying and mitigating some of the biases due to acquiescence. And the IS measures appeared less prone to meth odological biases in the first place. Collectively, these results show that we should not ignore the presence of acquiescence bias in our Likert scales and that we can easily implement a variety of cor rectives that can limit the errors that these scales produce. 
5.2. The toolkit for better social media survey measurement 
There are two types of approaches that researchers can use to mitigate acquiescence response bias. The first seeks to avoid bias through study design. This includes balancing scales as well as an approach that relies on “item-specific” measures that are less prone to acquiescence. The second is an attempt to statistically adjust for it where problematic questions have been used. This approach in cludes method factor models and ipsatization.13 For obvious rea sons, we believe it is better to avoid these biases in the first place than to need to statistically correct for them. Even the best statis tical corrections come at the cost of some loss of information about the construct of interest. In the next few paragraphs, we discuss some of the advantages and disadvantages to each approach, which are also summarized in Table 6. 
When researchers have the chance to design scales, at a mini mum, they should attempt to create balanced measures. These measures limit the potential for acquiescence and other method biases to undermine inferential statistics (Saris et al., 2010). Creating these scales is often relatively painless. Researchers can often reverse an item and ask a similar question about that reversed item. In some cases, however, the wording may become awkward; it is no small feat to reverse, “Facebook is part of my everyday ac tivity” such that “strongly agree” would be associated with less use. Balanced scales have the advantage of undermining relations be tween acquiescence and the latent variable of interest, but they do 
13 Aside from these, another less-used procedure, ipsatization correction, as shown in Online Appendix H reduced the inflation in reliability measures stemming from acquiescence, and we can be confident this procedure removed acquiescence impact because we do not see a similar drop in reliability for the balanced scale in our analysis. 
not technically eliminate the bias. These scales function by turning acquiescence into a sort of stochastic error, whereby serial acquiescers receive middling scores on the scale regardless of their true attitudes (Billiet & McClendon, 2000; Pasek & Krosnick, 2010). 
In general, we think that it makes more sense to attempt to eliminate the largest biases with item-specific measures as opposed to using measures with a known bias. And our evidence suggests that methodological biases with these measures are both smaller than those of the AD method and less concerning in their relations with criterion measures. We thus primarily recommend that future researchers design their studies with item-specific measures so that methodological biases are less likely to be pernicious. 
Among statistical correctives, method factor modeling provides the most efficient approach to eliminating acquiescence response bias. Its drawback is that it is relatively difficult to apply to many research settings. Specifically, a study needs to use a structural equation model to employ a method factor effectively. An alterna tive procedure called ipsatization can sometimes provide similar mitigation, but is more sensitive to measurement challenges (see Online Appendix H). 
Collectively, these approaches provide a solid foundation for improving self-report measures in social media studies. As should be apparent, no method is guaranteed to eliminate all sources of error. And researchers do not have the option of correcting issues at the design stage if they are using secondary data. In general, however, we believe that attempts to prevent acquiescence response bias are likely to be more effective than post-hoc cor rections. Additionally considering the pros and cons of each method in Table 6 in relation to specific research design (experi ment vs survey), sampling (telephone, online, mobile phone), and analysis (structural models, analysis of variance, multiple regres sion etc.) plans would also be important at the design stage of studies. 
5.3. Limitations and future research 
Our evidence in favor of IS measures is not conclusive. Particu larly, we find results suggesting that the IS versions of measures are not free from all method biases. Although assessments with these measures indicated no change in criterion validity when method biases were eliminated, the presence of methodological consis tency in these measures remains a potential concern and an important subject for future research. Indeed, the presence of this bias may explain some quizzical early results on the use of item specific measures as a means to address acquiescence elsewhere (cf. Saris et al., 2010). 
An additional concern is that the method factors we observe for the AD measures may not solely index acquiescence bias. Including the covariate measures of self-esteem and social capital should have helped to isolate measurement similarities, but the bias we observe could nonetheless represent positivity of the question or non-differentiation and satisficing for similarly presented items (MacKenzie & Podsakoff, 2012). Importantly, the use of latent var iables with SEM ensures that we are only examining the shared variance across items and are not simply overwhelming a few negatively worded items by averaging them with a large number of positively worded items. Nonetheless, it would be valuable for future studies to unpack the specific sources of methodological bias in both the AD and IS measures. Of particular interest is the role of personality differences such as agreeableness and how they might play out in acquiescent responding in the context of social media surveys. 
Another limitation was a direct product of attempts to match the 
O. Kuru, J. Pasek / Computers in Human Behavior 57 (2016) 82e92 91
Table 6 
Comparing remedies against acquiescence in social media measurement. 
Method Acquiescence approach Pros Cons 
Likert scale (doing 
nothing) 
Potential for considerable acquiescence bias 
Easily adapted to new measures of social media; no extra work 
Inflates and conflates the statistical findings; impact on validity of Facebook scales 
Balanced Scale Acquiescence bias canceled out; acquiescers placed at midpoint 
Relatively easy to apply at design stage of studies 
Acquiescent respondents scale scores biased towards midpoint; hard to apply to some Facebook use questions (reversed wordings can be awkward) 
Item-Specific Scales 
Acquiescence is not possible as agreement and disagreement are not response options 
Taps at the key construct of interest; eliminates bias at the source 
Sometimes difficult to determine what the key construct is; must be implemented at design stage 
Method Factor Acquiescence bias variance is taken out Great for SEM analysis; targeted measurement and removal of 
acquiescence bias 
Hard to employ for analytical procedures that do not use SEM e which is not common in Facebook studies 
Ipsatization (Online 
Appendix H) 
Acquiescence bias averaged out Easy to control for acquiescence and apply to any analytical procedure 
Consistent measurement of items is required, which may not always be easy for Facebook scales; can over-control when measures are not balanced 
IS formats with the original scales. The current study prized equivalent forms that remained loyal to the original meaning of the scales instead of perfecting the IS versions that were developed for experimental purposes. This preference meant that the IS scales did not always include optimal wording or perfect matches between the question stem and the response categories. Ideally, IS questions e like all other survey measures e should be designed to optimize principles of survey measurement in all domains (Pasek & Krosnick, 2010; Schuman & Presser, 1981). More research on how to best formulate item-specific responses would also serve to aide others hoping to avoid all sorts of measurement biases. We thus recom mend that researchers develop item-specific measures for their own scales that prioritize measurement of the key constructs of interest rather than adopting the specific wordings used here. 
Researchers might imagine that the effects of systematic biases such as acquiescence in social media scholarship are only marginal, in that they are unlikely to undermine our conclusions when we find large effects. This is indeed true, but we contend that it misses the point. For one, the presence of this bias is likely to occlude our assessments of small but important correlates. For another, it can introduce major challenges for factor analysis, when differing sets of measures will share more variance than they should, by stacking measurement of similar concepts with additional shared variance due to acquiescence. Further, the biases we observe are likely to compound one-another when we conduct meta-analyses, large national surveys, and field experiments within the social network sites (like those conducted by Facebook's internal data science research team) or when we incorporate measures into covariance based models such as SEM. In this manner, even a small acquies cence bias could lead to a large misinterpretation when aggregated. 
Of course, acquiescence is not the only bias that could skew our understanding of social media measurement. And it is likely that other methodological biases are operating in our measures and may be pernicious. Our focus on acquiescence should not minimize the relevance of other biases, such as social desirability bias, straightlining, or random responding most of which lead to sys tematic over-reporting. In this regard, these findings speak to Junco's (2013) comparison of self-report and behavioral data regarding time spent on Facebook as well. Although open-ended questions of time-spent do not have a structure that would trigger acquiescence bias, similar over-reporting biases were documented in the social media measures (Junco, 2013). But unlike many of these other forms of bias, assessing and correcting for acquiescence is relatively easy. Eliminating agreeedisagree response options obviates our concerns about this bias, and we can therefore improve our measures at relatively little cost. And there is little reason to think that a change to item-specific questions would make other biases worse. Future research should continue to assess 
other potential sources of bias in social media measurement. 6. Conclusion 
Social media measurement extensively relies on survey research and this means that traditional self-report problems can also in fluence social media research (Kuru & Pasek, in press; Junco, 2013). The current study focused on one such problem in order to demonstrate the systematic effects of even a small word choice and psychological tendency during survey response. We find evidence that the Facebook literature may be prone to just such a bias. At tempts to correct for acquiescence bias using method factors and IS response options show that there is a considerable and consistent impact of acquiescence and that it could be controlled for or pre vented by balanced scales, method factors, and item-specific question wordings. 
In understanding and quantifying the social media experiences of users, and in moving toward exploring causal mechanisms, it will be crucial to improve our measurement strategies. This requires social media researchers to pay serious attention to measurement and to employ toolkits against response biases such as acquies cence. Overall results indicated that item specific questions had better measurement properties than acquiescence-prone agreeedisagree measures. Indeed we find strong evidence of acquiescence bias for some the most commonly used Facebook measures. More research is needed to establish the importance and consequences of eliminating such systematic variance in Facebook research and for including IS question formats into the anti acquiescence toolkit composed of balanced scales, post-hoc method factor statistical corrections or standardization procedure. 
Acknowledgments 
Funding for this project came from a Winthrop B. Chamberlain Graduate Research Fellowship to O.K. by the Department of Communication Studies at the University of Michigan. 
Appendix A. Supplementary data 
Supplementary data related to this article can be found at http:// dx.doi.org/10.1016/j.chb.2015.12.008. 
References 
Bagozzi, R. P. (1984). A prospectus for theory construction in marketing. Journal of Marketing, 48(1), 11e29. http://doi.org/10.2307/1251307. 
Baumgartner, H., & Steenkamp, J.-B. E. M. (2001). Response styles in marketing research: a cross-national investigation. Journal of Marketing Research, 38(2), 143e156. 
92 O. Kuru, J. Pasek / Computers in Human Behavior 57 (2016) 82e92
Bentler, P. M. (1990). Comparative fit indexes in structural models. Psychological Bulletin, 107(2), 238. 
Bentler, P. M., & Bonett, D. G. (1980). Significance tests and goodness of fit in the analysis of covariance structures. Psychological Bulletin, 88(3), 588e606. http:// doi.org/10.1037/0033-2909.88.3.588. 
Berinsky, A. J., Huber, G. A., & Lenz, G. S. (2012). Evaluating online labor markets for experimental research: Amazon.com's mechanical turk. Political Analysis, 20(3), 351e368. http://doi.org/10.1093/pan/mpr057. 
Billiet, J. B., & McClendon, M. J. (2000). Modeling acquiescence in measurement models for two balanced sets of items. Structural Equation Modeling, 7(4), 608e628. 
Bode, L. (2012). Facebooking it to the polls: a study in online social networking and political behavior. Journal of Information Technology & Politics, 9(4), 352e369. http://doi.org/10.1080/19331681.2012.709045. 
Brown, P., & Levinson, S. C. (1987). Politeness: Some universals in language usage (Vol. 4). Cambridge University Press. 
Buhrmester, M., Kwang, T., & Gosling, S. D. (2011). Amazon's mechanical turk a new source of inexpensive, yet high-quality, data? Perspectives on Psychological Sci ence, 6(1), 3e5. http://doi.org/10.1177/1745691610393980. 
Carr, L. G. (1971). The srole items and acquiescence. American Sociological Review, 36(2), 287e293. http://doi.org/10.2307/2094045. 
Chan, W. (2003). Analyzing ipsative data in psychological research. Behaviormetrika, 30(1), 99e121. http://doi.org/10.2333/bhmk.30.99. 
Cloud, J., & Vaughau, G. M. (1970). Using balanced scales to control acquiescence. Sociometry, 33(2), 192e202. http://dx.doi.org/10.2307/2786329. Cheung, G. W., & Rensvold, R. B. (2000). Assessing extreme and acquiescence response sets in cross-cultural research using structural equations modeling. Journal of Cross-Cultural Psychology, 31(2), 187e212. http://doi.org/10.1177/ 0022022100031002003. 
Costa, P. T., & MacCrae, R. R. (1992). Revised NEO personality inventory (NEO PI-R) and NEO five-factor inventory (NEO FFI): Professional manual. Psychological Assess ment Resources. 
Couch, A., & Keniston, K. (1960). Yeasayers and naysayers: agreeing response set as a personality variable. The Journal of Abnormal and Social Psychology, 60(2), 151e174. http://doi.org/10.1037/h0040372. 
Ellison, N. B., Steinfield, C., & Lampe, C. (2007). The benefits of facebook “Friends:” social capital and college students' use of online social network sites. Journal of Computer-Mediated Communication, 12(4), 1143e1168. http://doi.org/10.1111/j. 1083-6101.2007.00367.x. 
Ellison, N. B., Steinfield, C., & Lampe, C. (2011). Connection strategies: social capital implications of Facebook-enabled communication practices. New Media & So ciety, 13(6), 873e892. http://doi.org/10.1177/1461444810385389. 
Ellison, N. B., Vitak, J., Gray, R., & Lampe, C. (2014). Cultivating social resources on social network sites: Facebook relationship maintenance behaviors and their role in social capital processes. Journal of Computer-Mediated Communication, 19(4), 855e870. 
Elphinston, R. A., & Noller, P. (2011). Time to face it! Facebook intrusion and the implications for romantic jealousy and relationship satisfaction. Cyberp sychology, Behavior, and Social Networking, 14(11), 631e635. http://doi.org/10. 1089/cyber.2010.0318. 
Fischer, R. (2004). Standardization to account for cross-cultural response bias A classification of score adjustment procedures and review of research in JCCP. Journal of Cross-Cultural Psychology, 35(3), 263e282. http://doi.org/10.1177/ 0022022104264122. 
Fowler, F. J. (1995). Improving survey questions: Design and evaluation (Vol. 38). Sage. Greenleaf, E. A. (1992). Improving rating scale measures by detecting and correcting bias components in some response styles. Journal of Marketing Research, 29(2), 176e188. http://doi.org/10.2307/3172568. 
Hu, L., & Bentler, P. M. (1999). Cutoff criteria for fit indexes in covariance structure analysis: conventional criteria versus new alternatives. Structural Equation Modeling: A Multidisciplinary Journal, 6(1), 1e55. http://doi.org/10.1080/ 10705519909540118. 
Jackson, D. N. (1959). Cognitive energy level, acquiescence, and authoritarianism. Journal of Social Psychology, 49(1), 65e69. 
Jarvenpaa, S. L., Tractinsky, N., & Saarinen, L. (1999). Consumer trust in an internet store: a cross-cultural validation. Journal of Computer-Mediated Communication, 5(2), 0e0 http://doi.org/10.1111/j.1083-6101.1999.tb00337.x. 
Junco, R. (2012). Too much face and not enough books: the relationship between multiple indices of Facebook use and academic performance. Computers in Human Behavior, 28(1), 187e198. http://doi.org/10.1016/j.chb.2011.08.026. 
Junco, R. (2013). Comparing actual and self-reported measures of Facebook use. Computers in Human Behavior, 29(3), 626e631. http://doi.org/10.1016/j.chb. 2012.11.007. 
Kline, R. B. (2005). Principles and practice of structural equation modeling, 2005. New York, NY: Guilford. 
Knowles, E. S., & Condon, C. A. (1999). Why people say “yes”: a dual-process theory of acquiescence. Journal of Personality and Social Psychology, 77(2), 379e386. http://doi.org/10.1037/0022-3514.77.2.379. 
Krasnova, H., Kolesnikova, E., & Guenther, O. (2009). “It Won't happen to me!”: self disclosure in online social networks. Amcis 2009 Proceedings, 343. Krasnova, H., Spiekermann, S., Koroleva, K., & Hildebrand, T. (2010). Online social networks: why we disclose. Journal of Information Technology, 25(2), 109e125. http://doi.org/http://dx.doi.org.proxy.lib.umich.edu/10.1057/jit.2010.6. Krosnick, J. A. (1991). Response strategies for coping with the cognitive demands of attitude measures in surveys. Applied Cognitive Psychology, 5(3), 213e236. 
http://doi.org/10.1002/acp.2350050305. 
Krosnick, J. A. (1999). Survey research. Annual Review of Psychology, 50, 537e567. Krosnick, J. A., & Alwin, D. F. (1987). An evaluation of a cognitive theory of response order effects in survey measurement. Public Opinion Quarterly, 51(2), 201e219. http://doi.org/10.1086/269029. 
Krosnick, J. A., & Fabrigar, L. R. (2001). Designing good questionnaires: Insights from psychology. New York: Oxford University Press. 
Kuru, O. & Pasek, J. (in press). Comparing social media use and political engage ment: Toward a valid measurement strategy. In Richardson G. (Ed.) Social media and politics: A new way to participate in the political process. Santa Barbara, CA: ABC-CLIO. 
Kwon, M.-W., D'Angelo, J., & McLeod, D. M. (2013). Facebook use and social capital: to bond, to bridge, or to escape. Bulletin of Science, Technology & Society, 0270467613496767. 
Leech, G. N. (1983). Principles of pragmatics. Taylor & Francis. 
Lee, E., Kim, Y. J., & Ahn, J. (2014). How do people use Facebook features to manage social capital? Computers in Human Behavior, 36, 440e445. http://doi.org/10. 1016/j.chb.2014.04.007. 
Lenski, G. E., & Leggett, J. C. (1960). Caste, class, and deference in the research interview. American Journal of Sociology, 65(5), 463e467. 
MacKenzie, S. B., & Podsakoff, P. M. (2012). Common method bias in marketing: causes, mechanisms, and procedural remedies. Journal of Retailing, 88(4), 542e555. http://doi.org/10.1016/j.jretai.2012.08.001. 
McKnight, D. H., Choudhury, V., & Kacmar, C. (2002). Developing and validating trust measures for e-commerce: an integrative typology. Information Systems Research, 13(3), 334e359. 
Mehdizadeh, S. (2010). Self-presentation 2.0: Narcissism and self-esteem on face book. Cyberpsychology, Behavior, and Social Networking, 13(4), 357e364. http:// doi.org/10.1089/cyber.2009.0257. 
Messick, S. (1991). Psychology and methodology of response styles. In Improving inquiry in social science: A volume in Honor of Lee J (pp. 161e200). Cronbach. Nunnally, J. C. (1978). Psychometric theory. New York: McGraw-Hill. Park, K.-G., Han, S., & Kaid, L. L. (2012). Does social networking service usage mediate the association between smartphone usage and social capital? New Media & Society, 1461444812465927. 
Pasek, J., & Krosnick, J. A. (2010). Optimizing survey questionnaire design in political science: Insights from psychology. In Oxford Handbook of American Elections and political behavior (pp. 27e50). 
Podsakoff, P. M., MacKenzie, S. B., Lee, J.-Y., & Podsakoff, N. P. (2003). Common method biases in behavioral research: a critical review of the literature and recommended remedies. Journal of Applied Psychology, 88(5), 879e903. http:// doi.org/10.1037/0021-9010.88.5.879. 
Rosenberg, M. (1965). Society and the adolescent self-image. Retrieved from http:// psycnet.apa.org/journals/ort/36/3/560.pdf%26productCode¼pa. Ross, C. K., Steward, C. A., & Sinacore, J. M. (1995). A comparative study of seven measures of patient satisfaction. Medical Care, 33(4), 392e406. Saris, W. E., Revilla, M., Krosnick, J. A., & Shaeffer, E. M. (2010). Comparing questions with agree/disagree response options to questions with item-specific response options. In In survey research methods (Vol. 4, pp. 61e79). 
Schriesheim, C. A., & Hill, K. D. (1981). Controlling acquiescence response bias by item reversals: the effect on questionnaire validity. Educational and Psycholog ical Measurement, 41(4), 1101e1114. http://doi.org/10.1177/ 001316448104100420. 
Schuman, H., & Presser, S. (1981). Questions and answers in attitude surveys. New York: Academic Press. 
Simon, H. A. (1957). Models of man; social and rational. Retrieved from http://doi. apa.org/psycinfo/1958-00363-000. 
Steinfield, C., Ellison, N. B., & Lampe, C. (2008). Social capital, self-esteem, and use of online social network sites: a longitudinal analysis. Journal of Applied Develop mental Psychology, 29(6), 434e445 http://doi.org/10.1016/j.appdev.2008.07.002. 
Vitak, J., Zube, P., Smock, A., Carr, C. T., Ellison, N., & Lampe, C. (2011). It's compli cated: Facebook users' political participation in the 2008 election. CyberP sychology, Behavior, and Social Networking, 14(3), 107e114 http://doi.org/ 10.1089/cyber.2009.0226. 
Weijters, B., Geuens, M., & Schillewaert, N. (2010). The individual consistency of acquiescence and extreme response style in self-report questionnaires. Applied Psychological Measurement, 34(2), 105e121. http://doi.org/10.1177/ 0146621609338593. 
Welkenhuysen-Gybels, J., Billiet, J., & Cambre, B. (2003). Adjustment for acquies-   cence in the assessment of the construct equivalence of Likert-Type score items. Journal of Cross-Cultural Psychology, 34(6), 702e722. http://doi.org/10.1177/ 0022022103257070. 
Williams, D. (2006). On and off the Net: scales for social capital in an online Era. Journal of Computer-Mediated Communication, 11(2), 593e628. http://doi.org/10. 1111/j.1083-6101.2006.00029.x. 
Williams, L. J., Gavin, M. B., & Williams, M. L. (1996). Measurement and non measurement processes with negative affectivity and employee attitudes. Journal of Applied Psychology, 81(1), 88e101. http://doi.org/10.1037/0021-9010. 81.1.88. 
Wilson, R. E., Gosling, S. D., & Graham, L. T. (2012). A review of facebook research in the social sciences. Perspectives on Psychological Science, 7(3), 203e220. http:// doi.org/10.1177/1745691612442904. 
Wise, L. Z., Skues, J., & Williams, B. (2011). Facebook in higher education promotes social but not academic engagement. Changing demands, changing directions. Proceedings Ascilite Hobart, 1332e1342.