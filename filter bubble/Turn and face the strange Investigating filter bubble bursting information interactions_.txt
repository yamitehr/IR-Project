 

City Research Online

CIT

UNIVERSITY OF LONDON

 

EST 1894

 

City, University of London Institutional Repository

 

Citation: McKay, D., Owyong, K., Makri, S. & Gutierrez Lopez, M. (2022). Turn and face

the strange: Investigating filter bubble bursting information interactions. In: UNSPECIFIED
(pp. 233-242). Association for Computing Machinery. ISBN 9781450391863 doi:
10.1145/3498366.3505822

This is the accepted version of the paper.

This version of the publication may differ from the final published version.

 

Permanent repository link: httos://openaccess.city.ac. uk/id/eprint/28089/

Link to published version: https://doi.org/10.1145/3498366.3505822

Copyright: City Research Online aims to make research outputs of City,
University of London available to a wider audience. Copyright and Moral Rights
remain with the author(s) and/or copyright holders. URLs from City Research
Online may be freely distributed and linked to.

Reuse: Copies of full iterns can be used for personal research or study,
educational, or not-for-profit purposes without prior permission or charge.
Provided that the authors, title and full bibliographic details are credited, a
hyperlink and/or URL is given for the original metadata page and the content is
not changed in any way.
 

City Research Online: http://openaccess.city.ac.uk/ publications @city.ac.uk

 
Turn and Face the Strange: Investigating Filter Bubble Bursting
Information Interactions

Turn and Face the Strange: Filter Bubble Bursting Behaviour

Dana McKay
School of Computing Technologies, RMIT University, dana mckay@rmit.edu.au

Kaipin Owyong
iSchool, University of Melbourne, kaipin.owyong@unimelb.edu.au

Stephann Makri
Centre for HCI Design, City, University of London, stephann@city.ac.uk

Marisela Gutierrez-Lopez

Faculty of Social Science and Law, University of Bristol, Marisela.gutierrezlopez@bristol.ac.uk

It is a ‘truth universally acknowledged’ that people prefer to minimize encounters with information they disagree
with and ignore it where they find it. Algorithms purportedly support this avoidance by creating filter bubbles filled
only with agreeable information potentially increasing polarisation and undermining democracy. How accurate is
this portrayal, though? Recent research has begun to cast doubt. We challenge these assumptions and report atwo-
phase analysis of filter bubble-bursting behavior. The first phase reports novel incidental findings from an interview
study on the role of information interaction in view change. Participants demonstrated a clear interest ina diversity
of information, including information specifically opposed to their own views. The second phase reports findings
from a diary study specifically designed to investigate people’s interactions with information that reflected a
different view to theirs. We examine how people found disagreeable information, how they responded to it and the
factors affecting their responses. We find that people will sometimes actively seek and engage with disagreeable
information, rather than avoid and ignore it. Our findings pave the way for future information interfaces that
support this previously undiscussed information interaction.

CCS CONCEPTS »* Information systems~Information retrieval~Users and interactive retrievaleHuman-centered
computing~Human computer interaction (HCI)~Empirical studies in HCI

Additional Keywords and Phrases: Filter bubbles, echo chambers, social media, view change, information interaction
ACM Reference Format:

First Author’s Name, Initials, and Last Name, Second Author’s Name, Initials, and Last Name, and Third Author's
Name, Initials, and Last Name. 2018. The Title of the Paper: ACM Conference Proceedings Manuscript Submission
Template: This is the subtitle of the paper, this document both explains and embodies the submission format for
authors using Word. In Woodstock ‘18: ACM Symposium on Neural Gaze Detection, June 03-05, 2018, Woodstock,
NY. ACM, New York, NY, USA, 10 pages. NOTE: This block will be automatically generated when manuscripts are
processed after acceptance.

1 Introduction

How much do those around us influence what we think? Can we be trapped into specific ways of thinking by
algorithms feeding us what we want, rather than what we need? These are the key questions posed by research
on filter bubbles and echo chambers respectively. Echo chambers are the limited information ecologies created by
surrounding ourselves with only those who are socially and ideologically similar to us, thus avoiding alternative
viewpoints [45]. Filter bubbles are the algorithmic extension of this idea; the personalisation of search engines
(or, more recently, social media feeds) exposes us to news and ideas we are more likely to find agreeable [8; 36].
Some have even argued that these information structures nudge us further and further from the centre on issues,
encouraging us to engage with only certain viewpoints and resulting in polarisation [8].

If these structures are as powerful as suggested, they present significant social challenges—having been
blamed for increasing climate change denial [4] and COVID vaccine hesitancy [38], and even influencing the
outcome of the 2016 US election [47].

The ability to engage with a diversity of information has been described as a cornerstone of democracy [18].
Seeing views other than one’s own may promote either debate or consensus [18]. Diverse information also affords
the opportunity to reconsider and possibly change one’s views [29]. Not getting this diversity of information,
conversely, deprives people of these opportunities for developing new perspectives, and possibly reduces
creativity and innovation as well [26; 28].

Popular discourse in this space suggests that people will avoid information diversity, preferring to stick within
their filter bubbles and echo chambers [3; 42]. Recent research, however, has begun to question whether filter
bubbles are as impermeable—or even as appealing—as this discourse suggests [8; 9; 21; 47]. Indeed, it is unlikely
any algorithm can insulate people from all information they disagree with [9], and unwillingness to engage with
alternative views may not be as entrenched as previously thought [8]. Regardless of their permeability, to break
out of a filter bubble, people must first see, and then engage with views different than their own. The core
question then, is whether they see and engage with alternative viewpoints, and if so, how?

In this paper, we present empirical evidence from two qualitative studies on people’s interactions with
information that reflected views different to their own. The first study identifies a propensity for deliberately
engaging with information from alternative viewpoints among a highly specialized group: people who had
recently changed their view on an important issue. Having both recently changed their view and being willing to
discuss it, this group is unlikely to be representative of the general population. The second study therefore
involved a broader group of social media users. It examines interactions with information reflecting alternative
views on social media, exploring how people acquire and react to that information. We demonstrate that aversion
to information reflecting alternative views, and avoidance of it where it is discovered, is far from universal. We
identify engagement with information from alternative or opposing viewpoints as a new type of broad
information need and identify factors that promote and inhibit this engagement. This paves the way for future
novel information interfaces that facilitate finding and engaging with alternative views.

The remainder of this paper is structured as follows: firstly, we discuss relevant background literature,
focusing on studies of filter bubbles and echo chambers (see Sec. 2), In Section 3, we report novel incidental
findings from our study of information-facilitated view change that motivated our dedicated study of peoples’
engagement with information they do not agree with. In Section 4, we explain and justify the study method,
presenting findings in Section 5. We discuss both studies in the context of existing literature, offering suggestions
for future information interfaces in Section 6. Finally, in Section 7, we draw conclusions and suggest avenues for
future work,
2 Background

In this section we discuss relevant background literature, focusing first on theoretical discussions around filter
bubbles and echo chambers, then on empirical examinations of information interaction around these information
structures.

2.10n the nature of filter bubbles and echo chambers

In 2001, Sunstein argued that despite the internet being a wellspring of information, it may have a deleterious
effect on democracy by allowing people to increasingly engage only with likeminded others, avoiding news and
opinion they find offensive or disagreeable—a concept he referred to as information cocoons or echo chambers
[45]. Recently, Sunstein has extended this argument to social media, arguing it is exacerbating the problem
described 20 years ago [44].

Ten years after Sunstein first mooted echo chambers, the idea of the filter bubble—algorithms that hide
information they think we don't want to see by personalising our search results—was popularized by Pariser
[36]. While Pariser focused on search, other commentary on filter bubbles focuses on social media feeds [8].
Unlike search, these feeds are not constrained by keyword relevance, and thus social media algorithms select
what we see based entirely on engagement and personalisation. This has been raised as a particular concern due
to the increase in use of social media for news access [2; 6].

The suggestion that the internet may hide things from us, potentially allowing messages we agree with to
become ‘consistent and overwhelming’ [35] (like effective propaganda) has attracted rapid and significant
attention from ethico-legal scholars [17-19]. Democracy relies on an informed populace to make voting decisions,
so information interfaces and systems that limit or bias information are viewed with great caution [17]. Based on
concerns about the impact of limiting information to a single perspective, these same scholars have argued for
policy measures to increase diversity in search results and social media recommendations [18]. One challenge
they identify is how to measure diversity [19]; this echoes similar discussions of the challenges of measuring bias
in news media [41].

More recent discussions of filter bubbles and echo chambers have pointed out that homophily—choosing to
associate predominantly with others like oneself—is both natural and potentially beneficial [9]. Benefits, if any,
stem from the ability to tease out the nuances of an argument with like-minded people, and that this process is
critical to advancing our thinking [17]. These recent discussions have questioned whether social media does
result in echo chambers, pointing out that ‘context collapse’ (where one’s social groups become a single audience
[5]) may mitigate against homophily. Similarly, they question whether algorithmic filter bubbles make a
meaningful difference to information ecologies, noting that it is very hard to entirely avoid information we
disagree with online [9]. This work also strongly questions the empirical evidence for filter bubbles; a question we
examine in section 2.2.

Anotable commonality of previous work is that it treats filter bubbles and echo chambers as though they are
likely to be innately desirable to individuals interacting with information, albeit potentially harmful to society at
large. In this paper, we question this assumption by focusing not just on peoples’ (putative) attitudes, but also on
their behaviour when they come across information they disagree with.

2.2 Information interactions with filter bubbles and echo chambers

While there has been considerable theoretical concern about filter bubbles and echo chambers, there is less
research on how these structures influence information interaction in practice, particularly in terms of people’s
experience of information.

Much of the evidence on the impact filter bubbles and echo chambers doesn’t interrogate which of these
structure(s) are responsible for the putative effect on people’s engagement with information. In fact without
significant knowledge of the underlying (proprietary) algorithms this would be difficult in practice; it is difficult to
determine what people might have seen (or not seen) without knowing how the algorithms work. Instead,
research examines whether there are divisions between groups of people, and whether those people are willing to
engage with information from the ‘other’ side, mostly focusing on news content.

General studies of news engagement demonstrate that political polarity does not affect which news sources are
engaged with: neither algorithmic nor social pressures push people away from the central news sources
consumed by the majority [16; 47]. Looking specifically at social media, researchers have found clear social
divides and strong evidence of homophily, but information is shared between groups [11]. Even when looking at
politically partisan news readers, there is a willingness to read information from ‘the other side’, and readers
spend longer on this type of news than they do on news with which they are more familiar [21]. The authors of
that work suggest this time is possibly spent looking for flaws or constructing counterarguments, however, rather
than engaging with an open mind. In contrast, our own previous work demonstrates that some people do
deliberately engage open-mindedly with information that reflects alternative views to theirs [29]. We thus aim to
provide a clearer picture of cross-ideological engagement.

While there is considerable evidence questioning the role of filter bubbles and echo chambers in view
formation and change, some studies do suggest they may be influential. A large-scale survey found that 90% of
news readers stuck within their political polarity, and that search engines and social media reinforced polarity
while aggregators reduced it [14], In this study, <5% of already-polarised participants engaged with news from
any source that was unlikely to reflect their own view, a finding supported by [6]. Another study similarly
demonstrated a low level of news engagement across political polarity on social media, with <20% of users
clicking on news they were likely to disagree with [3; 14]. While some of this was due to what algorithms showed
them, much was due to their own preferences. This finding is supported by a qualitative study of the echo
chamber effect ina single social network on Facebook which demonstrated that people were unwilling to engage
with alternative views, preferring instead to mute or ignore them [42]. Like people, algorithms are not unbiased:
a study of YouTube has demonstrated algorithms also facilitate a division of viewpoints [22]. However, algorithms
can also increase engagement with alternative views, as demonstrated by [48], which increased diversity and
changed terminology on search results reflecting alternative views ultimately making them demonstrably more
palatable to searchers.

Significant effort has gone into developing information interfaces that support engagement with alternative
views; much of which has focused on presenting a diversity of viewpoints and highlighting where these views
come from [6; 15; 24]. However, this research is based on a theoretical (rather than empirical) understanding of
filter bubbles and echo chambers,

Ultimately, the influence of filter bubbles and echo chambers on information interaction is unclear, but there
are research gaps. Little work has investigated whether information consumers want to engage with alternative
views (rather than examining what they do) and, if so, how best to design information interfaces to facilitate this
engagement. Similarly, only [42] has specifically examined the experience of engaging with information that does
not reflect one’s own view (in contrast with [33], which addresses reconciling opposing views), and only ina
single social network ona single platform. While some interfaces have been designed to support ‘filter bubble
breaking’ behaviour, they are not rooted in an understanding of that behaviour. In this paper we seek to
understand whether the dominant narrative, of a protective, desirable filter bubble and carefully curated echo
chamber, is what people really want. We examine how people come across information they disagree with,
whether and how they seek it out, and what they do when they meet it. Do they turn away, or do they engage, and,
if so, how and why?

3 Motivating Study

The genesis for the larger diary study reported in this paper was a new analysis of data from an interview study
on the role of information interaction in view change (see [29]). This analysis focused on the concepts of filter
bubbles and echo chambers, and findings have not been previously reported. We present them here as motivation
for the diary study described in the following sections.

3.1 Method for motivating study

The motivating study focused not on finding information that does not reflect one’s view, but on the broader role
of information interaction in view change. As more extensive details of the data collection approach have been
previously reported [29], we provide an overview to explain the novel analysis presented here.

We interviewed 18 people recruited from a variety of sources about their experience of changing view on an
issue that was important to them. We asked them to focus on the information interactions involved in the change,
and collected information about the view change process. Changes they described included changes of political
view, changing view on issues of personal importance (such as health or education decisions), becoming
vegetarian, changes of view on high profile criminal cases, and in one notable case, coming to believe the earth
may be flat.

Interviews were conducted in person or on Skype, and all but two were conducted by two researchers. They
were transcribed professionally, and analysed using an inductive thematic analysis approach [7]. In this paper, we
discuss the theme we identified on filter bubble breaking during view change. This theme surfaced without us
mentioning filter bubbles, echo chambers, or related ideas; it was entirely participant driven.

3.2 Filter bubble breaking during view change

Arguably, to break out of a filter bubble or echo chamber, you first must recognise that you are engaging witha
steady stream of similar information, and choose to change that. Participants expressed a high awareness of the
potential for filter bubbles and echo chambers to limit what they saw. For example, P8 noted ‘J have read about
these things that are about echo chambers and things like that, and people wanting to reinforce their views and not
willing to change it.’ Similarly, P9 said of himself I’m from a working class, white, non-English background in a very
rich country, so I’m sure that fits me into some kind of political bag and I’m sure some biases are inbuilt.’ Echo
chambers were seen as behaviourally-constructed, and something that participants could avoid if they so chose;
for example P11 stated ‘this sounds like a negative behaviour I need to avoid. So what can I do to do that?’. Other
participants suggested they were not stuck in echo chambers, for example P16 stated ‘I wouldn’t say my social
media is an echo chamber...1 have journalistic instincts and values...and I’m not one to get drawn into whatever.’ In
contrast, some participants, such as P14, saw value in being in an echo chamber to ‘discuss nuance’: ‘Well, it is fan
echo chamber] in one respect, because it's obviously a group of people who all share, at a more macro level, the same
political views. But on a more micro level, it's a safe place to discuss your thoughts’.

Alongside a recognition of the social factors driving their information engagement, participants recognised
algorithms were influencing some of what they saw. P10 said 7 have been following some people who talk about
climate change and reducing CO2 emissions. And because I follow these people then whenever I go to the explore part
of Instagram, | obviously see the suggestions for other pages on this’; similarly, P16 noted ‘there was some Twitter
algorithm which was like, you’re going to like all this US stuff, because you’ve been liking that a lot recently’.

This recognition of the limitations of their social circles and algorithmic feeds was a key driver for participants
wanting information from outside their filter bubbles or echo chambers; P11 noted there had ‘been so many
articles and other media about the echo chamber effect, and things like that’ that had made him aware of these
information constructs and their possible negative effects on view change.

Of 18 participants, 10 described occasions where they had deliberately taken action to expose themselves to
views other than their own, either regularly as part of their everyday information behaviour, or specifically on
an issue that had come to their attention. Some actions were intended to ensure access to a diversity of
information, others were specifically undertaken to ensure they saw positions they actively disagreed with.

Some participants mentioned searching for information they disagreed with: P17 said 7 think I Googled it
and talked to people as well, to try and understand why [people voted for Brexit]'. This participant deliberately
sought information diametrically opposed to her own view because she ‘wanted to understand’ the ‘other side.’
Other participants engaged with specific information sources they expected to contain views other than their
own. For example, P8 regularly read The Economist, even though her views were broadly left wing: ‘The Economist
is...very dogged religiously liberal free markets, and sometimes I think they skirt over the issues that have to do with
capitalism and globalisation. But I still read it because I like to keep an eye on what they're saying.’ Similarly, P9
consulted newspapers that were further right than him on the political spectrum to chance upon information he
knew he was likely to disagree with: ‘my political views are generally slightly left of centre...occasionally I go on the
news and get an article from like The Sun or something that’s quite right wing and I’ll just read what’s there just out
of sheer interest. While some participants specifically sought disagreeable information, others sought a plurality
of views, such as P11 who said ‘so at times it tends to be quite central. If | see a particular point of interest, I'll go to
The Guardian and The Times, and The Telegraph or The Mail just to see how differently those organisations interpret
something, and P2 who said of the people she followed on Twitter 7 don’t agree with all of them...but I don’t want
to be in this bubble where I only listen to people that have the same ideas as I do’.

While participants were open to alternative views, there were limitations to what they would engage with;
these limitations changed situationally. P14 noted that she would ‘never’ believe anything in the Daily Mail
because ‘they've got a massive agenda’. In contrast P2 noted that while generally she was prepared to engage with
alternative views to her own, ‘sometimes you feel pretty passionate about something. You cannot even think about
the idea that someone might disagree with you.’

In summary, participants reported a high level of awareness of the algorithmic and social limitations of
information in personalised environments, particularly on social media. In response to these limitations, many
took steps to ensure they engaged with information reflecting either a more diverse range of views, or views they
explicitly disagreed with. However, there were limits to this engagement based on both information sources and
participants’ personal views.

3.3 Motivation for further study

The narrative our participants expressed—awareness of being ina filter bubble or echo chamber—runs counter
to descriptions of participants in much previous research (for example [3; 14]). While the concept of the filter
bubble has occasionally been challenged, the idea that people congregate in social echo chambers, and that there
is a high personal cost for challenging these structures, is well supported by the literature [31; 42]. Furthermore,
while heralded asa social and political benefit, deliberate engagement with a diversity of views, or with opposing
views, has not been discussed in previous information interaction literature.

While the behaviour of participants in our motivating study was interesting, it raised as many questions as
answers. Firstly, our participants were relatively unusual: people who had both recognized a view change in
themselves, and who were willing to discuss it. This is often not the case [37]. Perhaps this openness also extends
to their information engagement. Secondly, self-reports of past behaviour may not be accurate [43]. Thirdly,
participants engaged with information that reflected different views to their own because they had deliberately
sought (rather than passively encountered) it, or at least created an information environment where encounters
were likely to occur [26].

We thus had several questions about filter bubble breaking behaviour: does it occur outside the context of
view change? What kinds of behaviour do people who come across information they disagree with engage in
when they are not looking for that information? Do incidental encounters result in view change, or even
engagement, or is the unwillingness to engage described elsewhere more typical? To investigate these questions,
we designed a diary study to capture encounters with information that reflects a different view to their own
relatively contemporaneously. The diary study focused on how they found this information, whether they
responded and how, the factors affecting their response and whether this prompted further interactions. We now
describe the method for this study.
4 Diary Study Method

Based on our previous study, where many encounters with view-changing information occurred on social media,
we wanted to understand what happened when people encountered information at odds with their view
(‘disagreeable information’). Outside of the context of view change, we had little understanding of how frequent
such encounters were, nor how they occurred, nor how people reacted to them. Diary studies have commonly
been used to study information experiences that may be infrequent and hard to subsequently recall [1; 27; 30];
we chose this approach. As with previous studies, we followed our diary studies up with semi-structured
interviews to clarify elements of the diary and elicit further information about participant motivations and
experiences [27]. Based on concerns about social media facilitating filter bubbles and echo chambers along with
our earlier finding that much disagreeable information is found on social media [29], we focused this study on
social media platforms.

We recruited 10 regular users of social media (who used at least one social media platform several times a
week) through a combination of social media advertising, personal contacts and snowball sampling. This number,
while small, is typical of other studies of information interaction (for example [11; 20; 27]), and allowed us to form
an initial impression of whether—and how—people engaged with disagreeable information. Nonetheless, the
sample size is a limitation of this study, as is our sampling approach: we advertised on our own social media, and
thus our participants may be ‘people like us’ who are more than usually willing to engage with disagreeable
opinions. We asked participants to refer other participants, and our social media feeds to repost our
advertisements. It, however, is unlikely that we reached very right-leaning people with deeply entrenched views,
who are less likely to engage with views other than their own [47]. Participants were aged 18-30; 5 men and 5
women. They reported using social media platforms, including Reddit, TikTok, Facebook, Twitter, YouTube and
Instagram. We refer to participants as P1-10.

4.1 Diary study

The diary period ran for one week in September 2020, during which participants were asked to take screenshots
of information they disagreed with on social media and describe their interactions with it. Diaries were recorded
in Google Docs; a new Google Doc was sent to participants at the end of each day, along with several prompts. The
first prompt in each diary was ‘did you see anything on social media that you disagreed with, or that changed your
view today?’ This question was followed by a series of lightweight prompts intended to elicit details about
participants’ interactions with disagreeable information, for example ‘what were you doing when you found this
information?’, ‘how did you feel when you saw it?’, ‘did you interact with it in any way, for example by commenting or
sharing?’, ‘what about this information didn’t align with your view?’. Participants were encouraged to use the
prompts as a starting point to provide a rich description of their information interactions. All but one participant
provided at least one diary entry. Participants recorded 48 initial interactions in their diaries; these could seed
further interactions, such as reading comments ina discussion thread or seeking follow-up information. 7
participants reported 3-6 initial interactions each; one reported 13 and one just a single interaction. The
remaining participant reported no interactions with disagreeable information.

While we analysed the diary entries in their entirety, we do not include the screenshots provided by our
participants in our results; complete anonymity is not possible due to photos, names and post searchability.
Reporting social media data in a way that individuals—particularly non-participants—can be identified is poor
ethical practice [40; 46].

4.2 Follow-up interviews

Semi-structured follow-up interviews were conducted over Zoom within a week of the diaries being completed.
Interviews lasted between 7 and 69 minutes, most around 45 minutes. The 7 minute interview was with P8, who
didn’t record any diary entries; we interviewed him briefly to confirm he had not seen any disagreeable
information, and ask why he thought that was.

Each participant's interview focused on their diary entries, using them as memory aids to elicit further detail
about their interactions with disagreeable information. Our questions focused on understanding ambiguities in
the diaries, the nature and strength of participants’ views, how participants found the information recorded in
their diaries, how they responded to it and what influenced decisions around those responses. Examples of
specific questions we asked included: ‘had you gone to Reddit with any specific purpose in mind?”, ‘did you interact
with this information in any way?’, ‘why did you look at the comments underneath this tweet?’ and ‘did you have any
strong feelings about anything highlighted in this before you saw it?’ Interviews were transcribed automatically,
then corrected by hand

4.3 Analysis

Diaries and interview segments were combined to form a single narrative for each participant, and analysed using
thematic analysis [7]. Analysis was done manually on paper by a single researcher; a second researcher conducted
an independent inductive analysis sensitised by the themes from our preliminary study. The resulting themes
describe interactions with disagreeable information, and factors affecting the willingness to engage with
disagreeable information. These themes are presented next. In line with Braun and Clarke’s assessment that the
number of times a theme occurs does not reflect the importance of the theme, we deliberately do not give
specifics of the number of instances of any theme or code [7]. In keeping with Braun and Clarke's approach, we
use quotations from participants to demonstrate each code, and codes are all bolded in their initial appearance.

5 Diary study findings

This section describes how participants found disagreeable information, their responses and the factors driving
and inhibiting these responses. Finally, we discuss participants’ reflections on the information sources, platforms
and algorithms they used when they found this information.

5.1 Finding disagreeable information

None of the experiences of disagreeable information our participants described began with search; instead, they
involved purposeful browsing and passive information encounters. This is perhaps unsurprising, given most
social media platforms incorporate feeds that promote information encounters [28].

Nevertheless, there were times where participants deliberately sought out information they knew they were
likely to disagree with. Sometimes this was part of a regular routine, such as P3 reading a long Reddit thread
about Melbourne each night, knowing it was likely to present a range of views about lockdown, or P1 who
regularly read a Reddit thread (r/ChangeMyView) ‘because there are really interesting posts on there. Very
contentious topics as well, so it’s interesting to read what other people’s views are’. Participants also found
disagreeable information when they deliberately engaged with contentious information, such as P1 seeking
out a subreddit about COVID cases in Melbourne while watching a TV update ‘to see how other people were
reacting’, or P3 selecting ‘very intensely downvoted or upvoted posts’ to ‘maybe just take a little look... to see what
content there is in those comments’.

Passive information encounters with disagreeable information tended to happen where participants were
just scrolling’ through social media feeds, ‘taking a break’ (P2) or ‘before I go to sleep’ (P4). Disagreeable
information encountered under these circumstances included a ‘slutshaming’ TikTok video (P4) and anti-
lockdown commentary from friends and associates (for example P3 and P9). Some disagreeable information in
these encounters was promoted by people outside participants’ social networks, such as comments on a post by
Australian politician Dan Andrews, which prompted P2 to state ‘I don’t know how Facebook works’ or a video P1
found ‘just on my YouTube homepage’. More commonly, though, it was shared by people participants knew, either
someone on the periphery of their lives (for example the Instagram influencer on whom P5 did a ‘deep stalk’) or
someone from their past. Referring to a disagreeable Facebook poster, P3 explained ‘this guy was in primary
school...at the end of primary school you add everyone on Facebook’

5.2 Responses to disagreeable information

One response to information participants did not agree with was disengagement—such as P3 who ‘laughed a
little bit and scrolled on’ after seeing an offensive post on Grindr or P6 who Just left and continued scrolling’ after
seeing something she disagreed with on Twitter. We saw this response less often than we might expect, but this is
likely because scrolling past disagreeable information on the internet is unlikely to be noteworthy or memorable.
In two cases participants disengaged more actively, unfriending people who posted things they disagreed with.
For example, P3 unfriended someone who posted anti-lockdown sentiment after seeing too much of it: ‘I’d just had
enough’. Some disengagement was more public, such as P9 who posted about anti-lockdown views ‘if any of you
guys do hold those views, just unfriend me now’.

Another common response to disagreeable information was reading comments or replies. One example of
this was P5, who when someone started ‘retweeting all the replies that agreed with her,’ ‘started looking at threads
and reading replies’ because ‘I like to know what people are thinking’. Similarly, P2 ‘read the replies’ toa post ona
state leader's Facebook post to see ‘what other people had to say.

Four participants engaged in active information seeking after they encountered disagreeable information.
This could be because they did not understand something about the view, for example P1 who didn’t know what
sunk cost fallacy was, so I had to Google that’. Others needed more information to understand what was being
expressed, such as P3 who when he saw Indian President Modi being compared to Trump and Bolsonaro ‘wiki’d
[sic] him a little bit’. Finally, some participants engaged in information seeking to test their own views, such as P10
who, when she saw a post favourably comparing the Swedish COVID response to lockdown, ‘took a little bit of time
to research and look at articles’.

Another response our participants engaged in was ‘liking/disliking’ using social media mechanisms. P1
reported disliking ‘to show the OP that people disagree with them’, whereas P3 wanted to let ‘other people know the
view is discouraged’. In contrast, P1 liked comments to ‘[show] the commenter and people reading the comments
“hey, this is a good comment”. Participants often liked or disliked in preference to commenting because it was
easier; P6 said: ‘T would have had to make an account to comment...otherwise you can just upvote or downvote’.

While less common, some participants did report sharing their own views. Some did this by commenting on a
post they disagreed with. For example, P9 replied directly to opposing posters to ‘debunk their opinion’ when
discussing a COVID-related arrest in Melbourne. In contrast, when P7 came across an anti-China post ona friend’s
WeChat, he did not want to confront the friend directly. However, he did post his views on his own feed, saying
people can just ‘pass it on [sic, meaning pass by] if they're not interested. As well as sharing publicly, P9 reported
he had shared his views on a group chat of likeminded friends because ‘it felt like it was a safe place to rant’,
whereas if he commented publicly he would ‘have to form a more coherent and logical argument’. In contrast to
these participants who shared their views, eight participants explicitly reported not sharing their views, such as
P3 who described himself as ‘a very passive consumer’ and P4 who said ‘in general I just lurk’.

Finally, albeit rarely, participants reported changing their views in response to disagreeable information they
encountered. One example came from P7, who reported deciding to ‘work harder on getting a job’ after reading a
post from an old school friend, where his original view was ‘I got time, like, one more year to graduate’. P5 also
reported changing her view, in this case about laws around payment for news content in Australia. At first, she
was concerned: ‘the way Google phrased it [the laws] were gonna change the way we search’, but then she talked to
a friend who said ‘nah nah, this is good, we need them to pay [writers] money,’ so she went from being ‘on Google’s
side but then I changed to the government's side’.
5.3 Factors affecting engagement

Participants reported a variety of both intrinsic and extrinsic factors that affected their engagement with
alternative views. Some of these factors affected their willingness to read or watch content with which they
disagreed, others affected their willingness to express an opinion, and two factors affected both.

5.3.1 Motivations to read disagreeable information.

One of the most common reasons for reading information participants knew they were likely to disagree with was
seeking controversy. One example came from P10, who decided to read the comments ona poll ‘because I knew
they'd be spicy’. Similarly, P3 examined ‘very intensely upvoted and downvoted’ comments on a Reddit subforum.
One reason participants gave for seeking controversy was understanding the spectrum of views on an issue.
For example, P2 read the comments on Dan Andrews’s post because there were ‘people hearting or whatever,
some people were angry or laugh reacting...so I really was curious as to what are people actually posting on his
page? How many people are actually mad at him?’. Similarly, P10 ‘wanted to understand how many people were
actually against this’ ina discussion of Melbourne’s lockdown and P6 wanted to ‘get, like, a general idea of what
Australia’s opinion was’.

Another motivating factor for participants to engage with disagreeable information was to evaluate their own
views. One approach was to compare their own views to those of others, such as P4 who said ‘I kind of want to
see what other people are thinking, the same as me or differently’ and P1 who wanted ‘to see where I sit compared
with other people’. Similarly, P5 wanted to ‘verify again, like if what I’m thinking is correct or just to like know what
the other side is, just to become more aware of the situation’. In other cases, participants were seeking validation
of their views, for example P6 who said ‘even though it’s kind of a controversial opinion [American racism being
different from racism elsewhere], | would like to see other people like backing it up, and making me feel like my
opinion is validated’. Similarly, P3 stated ‘people having the same kind of feeling, makes my feelings more valid’.

In line with wanting to validate their own views, two participants reported consuming disagreeable
information so they could construct a counterargument. P9 stated ‘so by getting like opposing information and
seeing what they were arguing and the points they were making I was able to like, think of those points and then
come up with counters, to their points’. This approach was supported by P10 who stated ‘to make your own
arguments, you need to know the other side’.

Finally, participants were sometimes genuinely interested in understanding and empathizing with those
who held a different view to them. P8, who did not make any diary entries because he did not find any
disagreeable information, described himself as someone who ‘liked to understand both sides of an issue’, and when
someone posted something he did not initially agree with, he tried to ‘see things from their point of view’ and
question ‘whether I need to broaden my own views’. Similarly, P10 said it was important to her to ‘be empathetic
that people’s opinions come from somewhere’, and P3 said he ‘could probably empathise a bit with where that person
was just coming from in terms of people wanting freedom’ in response to a post criticizing Melbourne’s lockdown,
even though he personally supported it. Even P9, who read others’ views to support constructing
counterarguments, commented that doing so ‘did open me up to, this whole other opinion and the arguments they
were making’.

5.3.2 Willingness to express an opinion.
Participants reported a variety of factors that influenced their motivation for sharing their own views, either in
the form of liking or disliking comments, or posting a comment of their own.

One of the major factors in whether participants sought to express their own views was their relationship
with the people posting. P9 noted that personally knowing the people posting content (rather than it being
promoted by an algorithm) made him more likely to read it. He stated if] had just come across it normally while
scrolling I probably would’ve scrolled past, but because it was shared by two of my friends, I was like wait, what is
this?’,

10
Relationships also influenced the way people reacted to posts. P7, for example, reported liking a post by
someone he wanted to reconnect with because ‘/ actually just want to draw some attention from him’. P8 noted
that he avoided interacting with disagreeable information to preserve relationships ‘I’d hate to destroy a relation
[sic] over one silly argument’. Whether the people posting were friends was another factor affecting decisions to
interact, for example, P2 said ‘when it’s a stranger, does it really matter if one more anonymous person likes it?’ and
‘T don’t care about arguing with someone who disagrees with me on the internet...no-one’s mind gets changed’. P4
reflected this distinction, saying ‘if it’s someone I know then I care more about sharing my opinion and hearing what
they have to say. If it’s people on the internet, it’s kind of like, well, people think some stuff and I think some stuff. P9
enjoyed debating with strangers on the internet but noted that, with strangers, his arguments needed to be
clearer and better formed.

Disagreeable information that was perceived as serious, or which had a strong emotional impact was more
likely to spur participants to engage or express their opinions. P2, for example noted she swiped through a post
about chihuahuas ‘because it was lighthearted, and not as serious’, whereas after reading a post about COVID, she
‘spent a bit more time reading the thread and googling [sic]’. Similarly, P9 who did not initially share his views ona
planned lockdown protest because he thought ‘oh, it’s just like a fad, it’ll pass’ did share his opinion by the end of
the week when it became apparent the protest was actually going to happen. The seriousness of the topic could
lead to a strong emotional reaction, such as for P1, who discussed a post about COVID support payments that
made him ‘more upset than some of the others.’ in turn making him ‘more inclined to react’. Similarly, when P9 saw
a disagreeable article repeatedly, he became irritated and posted because ‘I’ve had it up to here’.

Participants wanted to see representation of their views in discussions, and this affected their engagement.
If their opinion had not been represented, this could motivate them to post, such as self-described ‘lurker’ P4, who
described one of her reactions as ‘what the hell? And people were agreeing with it as well, and I was like someone
has to say this [to disagree]’. Similarly, P9 said of anti-lockdown arguments ‘7’ve had enough with those posts and
arguments, | wanna say something and put forward a different opinion’. Conversely, if participants felt their views
were well represented, they were less inclined to react to posts. P2 said ‘when I see someone’s already commented
it’s kind of like Oh, you know, they’ve already done it, so I don’t have to’. Similarly, P3 didn’t ‘angry react’ to a Grindr
profile that had already received several angry reacts: ‘the marginal benefit of me reacting additionally on top of
that’s not really gonna...have an impact or anything’.

Participants sometimes expressed an opinion to try to influence others, such as P7 who posted in response to
anti-China posts to ‘influence [the poster] to think another way’. P2 thought it was important to challenge
disagreeable information ‘for the good of all of us, [so] things that I think are incorrect or disagreeable won't just go
unchallenged...other people will just accept them at face value’.

Presentation of self was a key factor in decisions to express an opinion. P7, for example, feared creating an
enduring opinion profile that may have future consequences: ‘I just wanna stop generating these traces. In the
future, you know, something will happen and it will leave all these really bad histories for me’. Similarly, P2 said ‘I
don’t like connecting what goes on on the internet to myself, and P9 said I think because we have...family and friends
fon Facebook], we generally don’t want to share controversial opinions’. Sometimes though, self-presentation
motivated people to act. P3 unfriended someone because ‘ don’t wanna be associated with it [anti-lockdown
sentiment]’, and P8 ‘didn’t want to be known as a social media drama queen’, so avoided posting.

Related to presentation of self is the attention management some participants reported. Some avoided
posting opinions to avoid ‘get/ting] attacked’ (P7). P6 said she ‘just would never like post anything publicly...it really
freaks me out’. Others did not share their opinion because they felt it would be drawing attention to the opposing
view, such as P10 who said ‘you guys would totally see me react, and nah I’m not interested in that’.

Finally, and most prosaically, interface design motivated some decisions to express an opinion. P9 noted he
did not comment on TikTok ‘because it has a character restriction, and like, it’s very hard to get your opinion out’.
Similarly, P4 did not comment on Reddit because she ‘would’ve had to make an account to comment’. Similarly, P3
noted it was too much effort to react to a post he disagreed with ‘/’m lazy—it’s an additional function that I would

11
have to do’. This suggests that it is possible for design to influence this behaviour, also reflected in participants’
comments about the sources, platforms and algorithms they used when they found this information (see 5.4).

5.4Sources, platforms and algorithms

Participants reflected not only on their interactions with information they disagreed with, but also on the
information sources, platforms and algorithms they used when they found it.

Participants routinely reported using different platforms for different purposes, for example P3 stated
‘Instagram is a platform for me to connect and share...not a platform for me to look at political content’. Platforms
were not used in the same way by all participants, though. P6, for example, used Facebook to keep up with people
she knew personally, whereas on Twitter (where she found most of her disagreeable information): ‘7 just follow
like, random people who are fans of like the same artists that I am, or that sort of thing, so I tend to just get like
completely random opinions of people like on the other side of the world’. In contrast, P9 used Facebook to ‘follow,
like, all those news articles a bit more, so I’ve got more exposure to those different opinions and views and more
serious matters from Facebook’. Similarly, P4 described Reddit as ‘probably the most predictable...{for] something
on the front page, I'll usually agree with the comments’.

One common theme was bias. Three participants described certain news outlets as biased, such as P10 who
described 9News in Australia as ‘trash...not informative journalism’ and P3, who did not want information filtered
by some sort of journalist or something’. Participants also recognised communities as biased, for example, P4 who
described the commenters on 7 News as ‘don't know, like 52 year old boomers on their iPad’ and P6 who described
people she followed on Twitter as outwardly... feminist and that kind of thing. I think you call them Social Justice
Warriors’.

Finally, participants recognised the role of algorithms in what they were seeing, for example P4 who was
annoyed that after she watched something she disagreed with, TikTok kept showing her more of the same
because ‘the algorithm sorts you into which posts you might like’. P10 said of Facebook that ‘you may not subscribe
to these people, or these pages, but according to maybe the algorithm of what you've been looking at, what you’re
spending most of your time with, what makes you react longer, so, | assume it has a timer on me interacting with
these certain posts, so it will keep feeding me more of those posts, ‘cause it keeps me on Facebook’. Even where
participants were unsure of how algorithms worked, they hypothesised about their impact. P2, for example stated
‘T think it was a friend of mine liked it or commented on it which is why it got recommended...I don't know how
Facebook works anymore’.

5.5Summary

All but one of our ten participants interacted with information they disagreed with somewhere on social media
over the course of a week. This information was predominantly passively encountered, though participants
sometimes engaged in purposeful browsing to find it. Interactions with disagreeable information varied, but
included reading the comments, sharing one’s own view, disengaging and looking for more information. Factors
that affected participants interactions included relationships with the people posting information, the importance
or emotional weight of the topic, wanting to understand and evaluate the opinions of others, and the interface
limitations of various platforms. Finally, participants were acutely aware of many of the factors information
researchers are concerned about, such as algorithmic influence and bias.

6 discussion

One of the key contributions our work makes to the literature is on the permeability of filter bubbles. Previous
studies have attempted to quantify this permeability [14; 47; 48]. For a filter bubble (or echo chamber) to be
permeable, though, participants have to be willing to engage with information they find disagreeable, and
participants’ views on filter bubbles and echo chambers support this view. We have provided a qualitative

12
description of behaviour that—at least in the view of our participants—is designed to break filter bubbles.
Participants in both studies reported in this paper contradicted popular narratives around filter bubbles and echo
chambers. Not only were they theoretically willing to engage with alternative views, but their behaviour
demonstrates that they do engage. It has been suggested that engagement across ideological lines occurs mostly to
construct counterarguments, rather than to engage [21]; our data belies this: only two participants mentioned
constructing counterarguments. Many more participants wanted to understand alternative views, or empathise
with the people who held them.

Where our participants’ experiences did align more with previous research, was on the issue of echo chambers.
Our participants reported not wanting to engage with strangers, for fear they would be shouted at or called ‘social
media drama queens’ (P8), but their relationships with posters also affected how they reacted. As in previous
literature, old friends were discarded, but relationships with current friends were protected, and a site for open
discussion [25; 42].

The echo chamber metaphor highlights a paradox in our results. Participants found the likes/dislikes and
comments they read useful, but were often loath to engage themselves, particularly in the form of comments.
While lurking is common [32], it reinforces filter bubbles and thus encouraging comments and discussion could
be considered positive. Participants themselves mentioned interface barriers to engagement (such as lengthy
sign-in processes) and reducing these barriers while maintaining the accountability that reduces trolling [49] isa
key challenge the future design of information interfaces. Participants were also (rightly) concerned about privacy
and self-presentation, though, and understanding the relationship between views presented online and
participants privacy concerns and self-concept is a complex challenge, ripe for further information interaction
research,

Where participants did post their own views, or react, they did so because they wanted to see their views
represented and influence others, a finding that is similar to work on misinformation [10]. This finding supports
theoretical work on filter bubbles, which posits diversity as a key defense against polarisation and division [18].
Similarly, posts disagreeing with misinformation have been shown to reduce trust in the misinformation [12], so
supporting opposing reactions may be an important protection against its spread. Some work on information
interfaces to show a diversity of viewpoints has already begun [6], but more work is needed on how best to
present these views. Creative thinking is needed, because diversity can promote backlash instead of engagement
[4; 23; 34]: how do we get the benefits without the backlash? Interfaces that support diversity would also support
one of the tasks our participants frequently engaged in: understanding the landscape of opinion and evaluating
their own opinion in relationship to it. Assessing one’s own beliefs and opinions relative to others has been shown
to reduce the spread of misinformation, and mitigate against polarisation [4] so supporting these tasks is
important, but again how we might do this remains unclear.

One final key lesson for information interaction in our data is the role of the passive information encounter.
Participants in our motivating study had structured their information environments to contain diverse
information (ideal for serendipity [26]), and none of the initial interactions with information from alternative
views described in this paper occurred through active search. This supports previous theoretical suggestions
about the importance of the role of serendipity in limiting the impact of filter bubbles and echo chambers [39].
Non-search information behaviour, serendipity and interfaces to support them are significantly under-researched
(13; 28], and our work reinforces the importance of understanding and supporting these behaviours.

7 Conclusions and future work

In this paper we report findings from two studies of information interaction with views different to, or
disagreeable to, those held by participants. The first study is a motivating study, based on a new analysis of
interview data collected to understand the role of information interaction in view change. This data demonstrated
that seeking out alternative views, or structuring an information environment where one regularly comes across
alternative views, are important, but previously unrecognized information tasks.

13
The participant group in the motivating study was inherently biased: people who had changed their views, and
thus were likely to have interacted with information that they disagreed with. To better understand what
‘normally’ happens when people interact with information that they disagree with, we ran a one-week diary study
with 10 participants, followed by interviews to clarify and expand on what participants had written in their
diaries. This study demonstrated that interactions with disagreeable information were common (all but two
participants reported at least three interactions), but that view change as a result is relatively uncommon (only 4
out of 47 interactions resulted in view change, each from a different participant). Participants typically passively
encountered, rather than actively sought, disagreeable information. Once encountered, participants sometimes
disengaged with disagreeable information. In contrast to the popular narrative, though, it was much more
common to engage further: participants reported reading comments, engaging in active information seeking,
evaluating their own views, expressing their own opinions, or—rarely—changing their view. Participants were
motivated to engage by a range of factors, including issue importance, representation of their own view, wanting
to empathise with ‘the other side’, and emotional impact.

This study has some limitations: participants were relatively young, the diary was for a short period (onlya
week), and the study focused on social media. Future work could address these gaps. The major contribution of
this work, though, is a new and rich qualitative understanding of how social media users find information they
disagree with, and how they react when they do: finding is less search driven, and reactions are less closed-
minded than previous research would lead us to expect.

Knowing that social media users actively engage with information different to their own views, and that this
sometimes affects their views, opens the door to future work designing and evaluating information interfaces that
support people who wish to engage with information even where it is disagreeable. This is a form of information
interaction that is previously unaccounted for in information interface design.: Information interaction research is
ideally placed to create new designs that support those who, like our participants, wish to turn and face the
strange.

Acknowledgements

We thank Google News Initiative and The University of Melbourne for financial support for the motivating study.
We also thank our participants, and George Buchanan for comments on a draft.

References

<bib id="bib1"><number>[1]</number> Adler, A, Gujar, A, Harrison, B.L, O'Hara, K,, and Sellen, A, 1998. A diary study of work-related reading:
design implications for digital reading devices. In Proc. CHI 98 (Los Angeles, California, United States), ACM Press, 241-248. DOI=

http: //dx doiorg/10.1145/274644.274679.

<bib id="bib2"><number>[2]</number> Australian Competition and Consumer Commission, 2018. Digital Platforms Inquiry: Preliminary Report.
https://www.accc gov.au/focus-areas /inquiries-finalised/digital-platforms-inquiry-0.</bib>

<bib id="bib3"><number>[3]</number>Bakshy, E., Messing, S., and Adamic, L.A, 2015. Exposure to ideologically diverse news and opinion on
Facebook. Science 348, 6239, 1130-1132. DOI= http://dx.doiorg/doi:10.1126/science.aaal160.</bib>

<bib id="bib4"><number>[4]</number>Benegal, S.D. and Scruggs, L.A, 2018. Correcting misinformation about climate change: the impact of
partisanship in an experimental setting. Climactic Change 148, 1, 61-80. DOI= http://dx.doiorg/10.1007 /s10584-018-2192-4.</bib>

<bib id="bib5"><number>[5]</number>Boyd, D., 2014. It's complicated: The social lives of networked teens. Yale University Press.</bib>

<bib id="bib6"><number>[6]</number>Bozdag, E. and van den Hoven, J., 2015. Breaking the filter bubble: democracy and design. Ethics and
Information Technology 17, 4, 249-265. DOI= http://dx.doiorg/10.1007 /s10676-015-9380-y.</bib>

<bib id="bib7"><number>[7]</number>Braun, V. and Clarke, V., 2006. Using thematic analysis in psychology. Qual Res Psych 3, 2, 77-101.</bib>
<bib id="bib8"><number>[8]</number>Bruns, A, 2019. Filter bubble. /nternet Policy Review 8, 4. DOI=

http: //dx doi.org/10.14763 /2019.4.1426.</bib>

<bib id="bib9"><number>[9]</number>Bruns, A, 2019. It’s not the technology, stupid: How the ‘Echo Chamber’ and ‘Filter Bubble’ metaphors
have failed us. In Prac. [AMCR 19 (Madrid, Spain), International Association for Media and Communication Research.</bib>

<bib id="bib10"><number>[10]</number>Chen, X,, Sin, S.-C.J., Theng, Y.-L. and Lee, €.S., 2015. Why Do Social Media Users Share Misinformation?
In Proc. JCDL 15 (Knoxville, Tennessee, USA), Association for Computing Machinery, 111-114. DOI=

http: //dx doiorg/10.1145 /2756406.2756941.</bib>

<bib id="bib11"><number>[11]</number>Colleoni, E., Rozza, A. and Arvidsson, A, 2014. Echo chamber or public sphere? Predicting political
orientation and measuring political homophily in Twitter using big data. ] Comm 64, 2, 317-332.</bib>

<bib id="bib12"><number>[12]</number>Colliander, J., 2019. “This is fake news”: Investigating the role of conformity to other users’ views when
commenting on and spreading disinformation in social media. Comp. Hum. Behav. 97, 202-215. DOI=

http: //dx doi.org/10.1016/j.chb.2019.03.032.</bib>

 

14
<bib id="bib13"><number>[13]</number> Fidel, R., 2012. Human information interaction: an ecological approach to information behavior. MIT
Press.</bib>

<bib id="bib14"><number>[14]</number>Flaxman, S., Goel, S., and Rao, J.M., 2016. Filter Bubbles, Echo Chambers, and Online News
Consumption. Public Opinion Q 80, $1, 298-320. DOI= http://dx.doiorg/10.1093 /pog /nfw006.</bib>

<bib id="bib15"><number>[15]</number>Foth, M, Tomitsch, M., Forlano, L., Haeusler, M.H., and Satchell, C., 2016. Citizens breaking out of filter
bubbles: urban screens as civic media. In Proc. PERDIS 16 (Oulu, Finland), Association for Computing Machinery, 140-147. DOI=

http: //dx doiorg/10.1145/2914920,2915010.</bib>

<bib id="bib16"><number>[16]</number> Guess, A, Lyons, B., Nyhan, B., and Reifler, J. 2018. Avoiding the echo chamber about echo chambers:
Why selective exposure to like-minded political news is less prevalent than you think. Knight Foundation https: //kf-site-
production.s3.amazonaws.com/media_elements/files/000/000/133 /original /Topos_KF_White-Paper_Nyhan_V1.pdf.</bib>

<bib id="bib17"><number>[17]</number>Helberger, N., 2011. Diversity by Design. |. Inf Policy 1, 441-469. DOI=

http: //dx doi.org/10.5325/jinfopoli.1.2011.0441.</bib>

<bib id="bib18"><number>[18]</number>Helberger, N., Karppinen, K,, and D’Acunto, L, 2018. Exposure diversity as a design principle for
recommender systems. Inf Comm & Soc. 21, 2, 191-207. DOI= http://dx.doi.org/10.1080/1369118X.2016,.1271900.</bib>

<bib id="bib19"><number>[19]</number>Helberger, N., Kleinen-von Konisgléw, K., and van der Noll, R, 2015. Regulating the new information
intermediaries as gatekeepers of information diversity. Information and Media 17, 6, 50-71.</bib>

<bib id="bib20"><number>[20]</number>Kefalidou, G. and Sharples, S., 2016. Encouraging serendipity in research: Designing technologies to
support connection-making. /JHCS 89, 1-23. DOI= http://dx.doiorg/10.1016 /j.ijhes.2016.01.003.</bib>

<bib id="bib2 1"><number> [2 1]</number>Kelly, G.R, 2009. Echo chambers online?: Politically motivated selective exposure among Internet news
users. ] Comp Mediated Comm 14, 2, 265-285. DOI= http://dx.doi.org/10.1111/j.1083-6101.2009.01440.x.</bib>

<bib id="bib22"><number>[22]</number>Ledwich, M. and Zaitsev, A, 2020. Algorithmic extremism: Examining YouTube's rabbit hole of
radicalization. First Monday 25, 3. DOI= http://dx.doi.org/10.5210/fm.v25i3.10419.</bib>

<bib id="bib23"><number>[23]</number>Lewandowsky, S., Ecker, U.K.H., Seifert, C.M, Schwarz, N., and Cook, J., 2012. Misinformation and Its
Correction: Continued Influence and Successful Debiasing. Psyc Sci in Public Interest 13, 3, 106-131. DOI=

http: //dx doi.org/10.1177/1529100612451018.</bib>

<bib id="bib24"><number>[24]</number>Liao, Q.V. and Fu, W.-T., 2014. Can you hear me now?: mitigating the echo chamber effect by source
position indicators. In Proc. CSCW 14 (Baltimore, Maryland, USA), ACM, 2531711, 184-196. DOI=

http: //dx doiorg/10.1145/2531602.2531711.</bib>

<bib id="bib25"><number>[25]</number>Lopez, M.G. and Ovaska, S., 2013. A look at unsociability on Facebook. In Proc. BCS HCI 13 (London, UK),
BCS Learning & Development Ltd, Article 13.</bib>

<bib id="bib26"><number>[26]</number>Makri, S., Blandford, A., Woods, M.,, Sharples, S., and Maxwell, D., 2014. “Making my own luck’:
Serendipity strategies and how to support them in digital information environments. JASIST 65, 11, 2179-2194, DOI=

http: //dx doi.org/10.1002/asi.23200.</bib>

<bib id="bib27"><number>[27]</number>Makri, S., Ravem, M., and McKay, D., 2017. After serendipity strikes: Creating value from encountered
information. ASIST Proceedings 54, 1, 279-288, DOI= http: //dx doiorg/10.1002/pra2.2017.1450540103 1.</bib>

<bib id="bib28"><number>[28]</number>McKay, D., Makri, S., Chang, S., and Buchanan, G., 2020. On Birthing Dancing Stars: The Need for
Bounded Chaos in Information Interaction. In Proc. CHIIR 2020 (Vancouver BC, Canada), Association for Computing Machinery, 292-302. DOI=
http: //dx doi.org/10.1145/3343413,3377983.</bib>

<bib id="bib29"><number>[29]</number>Mckay, D., Makri, S., Gutierrez-Lopez, M, MacFarlane, A., Missaoui, S., Porlezza, C., and Cooper, G., 2020.
We are the Change that we Seek: Information Interactions During a Change of Viewpoint. In Proc. CHIIR 20 (Vancouver BC, Canada), Association for
Computing Machinery, 173-182. DOI= http: //dx.doiorg/10.1145 /3343413.3377975.</bib>

<bib id="bib30"><number>[30]</number>McKenzie, P.J., 2003. A model of information practices in accounts of everyday-life information seeking.
J Doc 59, 1, 19-40. DOI= http: //dx.doi.org/10.1108 /00220410310457993.</bib>

<bib id="bib3 1"><number> [3 1]</number>Metzl, J.M., 2019. Dying of whiteness: how the politics of racial resentment is killing America’s
heartland. Hachette UK.</bib>

<bib id="bib3 2"><number>[32]</number> Nonnecke, B. and Preece, J., 2000. Lurker demographics: counting the silent. In Proc. CHI 00 (The
Hague, The Netherlands), Association for Computing Machinery, 73-80. DOI= http://dxdoiorg/10.1145/332040.332409.</bib>

<bib id="bib33"><number>[33]</number> Novin, A. and Meyers, E., 2017. Making Sense of Conflicting Science Information: Exploring Bias in the
Search Engine Result Page. In Proc. CHIR 17 (Oslo, Norway), ACM, 175-184. DOI= http://dx.doiorg/10.1145 /3020165.3020185.</bib>

<bib id="bib3 4"><number>[34]</number> Nyhan, B. and Reifler, J., 2010. When Corrections Fail: The Persistence of Political Misperceptions.
Political Behaviour 32, 2, 303-330. DOI= http://dx.doi.org/10.1007 /s11109-010-9112-2.</bib>

<bib id="bib3 5"><number>[35]</number>0'Donnell, V. and Jowett, G.S., 1992. Chapter 4. In Propaganda and Persuasion Sage, 122-154.</bib>
<bib id="bib36"><number>[36]</number>Pariser, E., 2011. The filter bubble: What the Internet is hiding from you. Penguin UK.</bib>

<bib id="bib37"><number>[37]</number>Petty, E., S.C., W., and Tormala, Z., 2003. Persuasion and Attitude Change. In Handbook of Psychology
Wiley, 353-382. DOI= http: //dx doi.org/10.1002/0471264385,wei0515.</bib>

<bib id="bib38"><number>[38]</number>Puri, N., Coomes, E.A, Haghbayan, H., and Gunaratne, K., 2020. Social media and vaccine hesitancy: new
updates for the era of COVID-19 and globalized infectious diseases. Hum Vaccin Immunother 16, 11, 2586-2593. DOI=

http: //dx doiorg/10.1080/21645515.2020.1780846.</bib>

<bib id="bib39"><number>[39]</number>Reviglio, U., 2017. Serendipity by design? How to turn from diversity exposure to diversity experience
to face filter bubbles in social media. In Proc. INSCI 17 (Thessaloniki, Greece), Springer, Berlin, 281-300.</bib>

<bib id="bib40"><number> [40]</number>Rivers, C. and Lewis, B.,, 2014. Ethical research standards in a world of big data F1000Research 3, 38.
DOI= http://dx.doiorg/10.12688 /f1000research.3-38.v2.</bib>

<bib id="bib4 1"><number>[41]</number>Robertson, RE, Jiang, S., Joseph, K,, Friedland, L., Lazer, D., and Wilson, C., 2018. Auditing Partisan
Audience Bias within Google Search. In Proc. CSCW 18 (Austin, TX), New York NY, 1-22. DOI= http: //dx.doi.org/10.1145/3274417.</bib>

<bib id="bib42"><number>[42]</number>Seargeant, P. and Tagg, C., 2019. Social media and the future of open debate: A user-oriented approach
to Facebook’s filter bubble conundrum. Doiscourse Context and Media 27, 41-48. DOI= http://dxdoiorg/10.1016/j.dcem.2018.03.005.</bib>

<bib id="bib43"><number>[43]</number>Sharp, H., Preece, J., and Rogers, Y., 2019. Interaction Design-Beyond Human-Computer Interaction
Wiley, Trenton, NJ.</bib>

15
<bib id="bib44"><number> [44]</number>Sunstein, C., 2018. #Republic: Divided Democracy in an Age of Social Media. Princeton University
Press.</bib>

<bib id="bib45"><number>[45]</number>Sunstein, C.R.,, 2001. Republic. com. Princeton University Press, Princeton, NJ.</bib>

<bib id="bib46"><number>[46]</number> Webb, H,, Jirotka, M,, Stahl, B.C., Housley, W., Edwards, A, Williams, M., Procter, R, Rana, 0., and
Burnap, P., 2017. The Ethical Challenges of Publishing Twitter Data for Research Dissemination. In Proc. WebSci 17 (Troy, New York, USA), ACM,
New York, NY, 339-348. DOI= http: //dx.doiorg/10.1145 /3091478,3091489,</bib>

<bib id="bib47"><number>[47]</number> Weeks, B.E,, Ksiazek, T.B., and Holbert, R.L, 2016. Partisan Enclaves or Shared Media Experiences? A
Network Approach to Understanding Citizens’ Political News Environments. J Broadcasting Elec Media 60, 2, 248-268. DOI=

http: //dx doi.org/10.1080/08838151.2016.1164170.</bib>

<bib id="bib48"><number>[48]</number>Yom-Tovy, E., Dumais, S., and Guo, Q., 2013. Promoting Civil Discourse Through Search Engine Diversity.
Soc Sci Comp Rev 32, 2, 145-154. DOI= http://dx.doiorg/10.1177 /0894439313506838.</bib>

<bib id="bib49"><number> [49]</number>Zannettou, S., Sirivianos, M., Blackburn, J., and Kourtellis, N. 2019. The Web of False Information:
Rumors, Fake News, Hoaxes, Clickbait, and Various Other Shenanigans. /, Data and Inf. Quality 11, 3, Article 10. DOI=

http: //dx doi.org/10.1145/3309699,</bib>

16
