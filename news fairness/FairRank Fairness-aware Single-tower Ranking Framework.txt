FairRank: Fairness-aware Single-tower Ranking Framework
for News Recommendation
Chuhan Wu1 , Fangzhao Wu2 , Tao Qi1 , Yongfeng Huang1

1 Department of Electronic Engineering & BNRist, Tsinghua University, Beijing 100084
2 Microsoft Research Asia, Beijing 100080, China

{wuchuhan15,wufangzhao,taoqi.qt}@gmail.com,yfhuang@tsinghua.edu.cn

arXiv:2204.00541v1 [cs.IR] 1 Apr 2022

ABSTRACT
Single-tower models are widely used in the ranking stage of news
recommendation to accurately rank candidate news according to
their fine-grained relatedness with user interest indicated by user
behaviors. However, these models can easily inherit the biases related to users’ sensitive attributes (e.g., gender) encoded in user
behavior data for model training, and result in recommendation
results unfair for users with certain attributes. In this paper, we
propose FairRank, a fairness-aware single-tower ranking framework for news recommendation. Since candidate news selection in
existing recommender systems can be biased, we propose to use
a shared candidate-aware user model to match user interest with
a real displayed candidate news and a random news. It learns a
candidate-aware user embedding that reflects user interest in candidate news, and a candidate-invariant user embedding that indicates
intrinsic user interest, respectively. We apply adversarial learning
to both of them to reduce the biases brought by sensitive user attributes. In addition, we use a KL loss to regularize the attribute
labels inferred from the two user embeddings to be similar, which
can make the model capture less candidate-aware bias information.
Extensive experiments on two datasets show that FairRank can
improve the fairness of various single-tower news ranking models
with minor performance loss.

KEYWORDS
News recommendation, Fairness, Single-tower
ACM Reference Format:
Chuhan Wu1 , Fangzhao Wu2 , Tao Qi1 , Yongfeng Huang1 . 2022. FairRank:
Fairness-aware Single-tower Ranking Framework for News Recommendation. In Proceedings of ACM Conference (Conference’17). ACM, New York,
NY, USA, 5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1

INTRODUCTION

Recall and ranking are two essential steps in personalized news recommendation [22, 25]. The recall step aims to select a small subset
of candidate news from the large news pool to comprehensively
cover different user interests [13]. In this step, two-tower models,
which learn user interest and news embeddings independently,
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

are common choices due to efficiency considerations [4, 9, 20, 24].
In contrast, the ranking step needs to accurately rank candidate
news according to their relevance with user interest [15]. Thus,
single tower models that can capture the fine-grained relatedness
between candidate news and user interest indicated by different
user behaviors, are widely used as the ranking algorithms in news
recommendation [6, 12, 30, 31].
News ranking models are usually learned on users’ click behavior data [8]. In fact, click behavior data inherently encodes biases
related to users’ sensitive attributes such as demographics [1, 5].
The model learned on such data can inherit and even amplify these
biases, which may generate biased recommendation results that
are unfair to users with certain sensitive attributes [23].1 Adversarial learning is a widely used technique to remove the biases
related to sensitive attributes from deep models to help make fair
predictions [3, 26, 28], which also has the potential to empower
fairness-aware news recommendation. For example, Wu et al. [23]
proposed a decomposed adversarial learning framework that learns
a bias-aware user model and a bias-free user model to capture bias
information and bias-independent user interest information for
news recommendation, respectively. However, existing methods
are based on two-tower architectures that model user interest and
candidate news independently, and the fairness issue of singletower news ranking models is rarely studied.
In this paper, we propose a fairness-aware single-tower ranking framework named FairRank for news recommendation, which
can effectively improve the fairness of various single-tower news
ranking models. Since the selection of candidate news in existing
recommender systems can be biased, in FairRank we propose to use
a shared candidate-aware user model to match user interest with a
real displayed candidate news and a randomly selected news, respectively, to learn a candidate-aware user embedding that indicates
user interest on a specific candidate news and a candidate-invariant
user embedding that encodes intrinsic user interest in terms of
expectation. To reduce the bias on sensitive user attributes encoded
by the model, we apply adversarial learning to both user embeddings to remove bias information. In addition, to further eliminate
bias information encoded by the selection of candidate news, we
propose to use a Kullback–Leibler (KL) divergence loss to regularize
the attribute labels inferred from the two user embeddings to be
similar, which can encourage the model to be less sensitive to the
inherent bias introduced by candidate news selection. Extensive experiments on two real-world news recommendation datasets show
that FairRank can improve the fairness of various single-tower
ranking models with minor performance sacrifice.
1 We find single-tower models may even have more severe unfairness than two-tower

models, see experiments.

Chuhan Wu1 , Fangzhao Wu2 , Tao Qi1 , Yongfeng Huang1

Conference’17, July 2017, Washington, DC, USA
Adversarial
Loss

Adversarial
Loss

Predicted
𝒛𝒛�
Attribute

Rec. Loss

Click Score

�
𝒚𝒚

Click
Predictor

KL
Loss

Predicted
Attribute

Attribute
Predictor

Attribute
Predictor

Dense

𝒛𝒛�

𝒅𝒅𝑐𝑐

Dense

𝒖𝒖𝑐𝑐

𝒅𝒅𝑟𝑟

𝒖𝒖𝑟𝑟

Candidate-aware
user embedding

Candidate-invariant
user embedding

Candidate-aware
Candidate-aware
Shared
User Model
User Model

𝐷𝐷𝑐𝑐

Candidate News

𝐷𝐷1

𝐷𝐷2

…

Clicked News

𝐷𝐷𝑁𝑁

𝐷𝐷𝑟𝑟

Random News

Figure 1: The framework of FairRank.

2

FAIRRANK

Next, we introduce the FairRank approach for fairness-aware news
ranking, as shown in Fig. 1. It incorporates a candidate-aware user
model to capture the interactions between clicked news and a candidate news or a random news, to model user’s interest in a specific
candidate news and candidate-invariant user interest (in the sense
of expectation). We then introduce its details and how to learn this
model to generate fair news rankings.

2.1

Candidate-aware Ranking Model

Since candidate news usually can only match parts of user interest
indicated by clicked news, modeling the fine-grained interactions
between clicked news and candidate news may achieve better news
ranking accuracy than directly matching candidate news with overall user interest. Thus, we use a candidate-aware user model to
capture users’ candidate-aware interest. It can be implemented by
many architectures, such as (1) using candidate-aware attention
to select clicked news according to their relevance to candidate
news [16]; (2) using 3-D convolutional neural network to match
candidate news and clicked news based on their texts [15]; and (3)
concatenating clicked news and candidate news into a long document as the input of a Transformer [14]. It takes a user’s clicked
news (denoted as [𝐷 1, 𝐷 2, ..., 𝐷 𝑁 ], where 𝑁 is the history length)
and a candidate news 𝐷𝑐 as input, and outputs a candidate-aware
user embedding u𝑐 . This embedding is further used to compute a
click score 𝑦ˆ for ranking based on the relevance between u𝑐 and the
hidden embedding of candidate news 𝐷𝑐 (denoted as h𝑐 ) learned in
the user model, which can be formulated as 𝑦ˆ = 𝑓 (u𝑐 , h𝑐 ), where
𝑓 (·) is a relevance function that is often implemented by inner
product or feedforward neural networks.

2.2

to the candidate-aware user embedding to reduce biases on sensitive attributes. Unfortunately, the candidate news list to be ranked
may also be biased, and it is difficult to purify the candidate-aware
user embedding. To address this challenge, we propose to use a
candidate-aware user model to jointly match clicked news with
a candidate news 𝐷𝑐 and a randomly selected news 𝐷𝑟 , to learn
a candidate-aware user embedding u𝑐 and a candidate-invariant
user embedding u𝑟 , respectively. Since the random news is uniformly selected from the entire news set, the candidate-invariant
user embedding can reflect the intrinsic user interest in the sense
of expectation. We further apply adversarial learning to both kinds
of user embeddings to learn debiased user interest. We use two
independent dense layers to learn hidden representations of them
(denoted as d𝑐 and d𝑟 ) for inferring the sensitive user attribute, and
a shared attribute predictor is used as the discriminator to predict
sensitive attribute labels (denoted as ẑ and z̃) as follows:

Fairness-aware Single-tower Ranking

We then introduce how to learn a fairness-aware single tower ranking model. A straightforward way is applying adversarial learning

ẑ = softmax(Wdc + b),

(1)

z̃ = softmax(Wdr + b),

(2)

where W and b are parameters in the discriminator. We use an
adversarial loss L𝐴 to train the discriminator by using the crossentropy comparing ẑ or z̃ against the attribute label. The adversarial
gradients of L𝐴 will be propagated to the user model to remove its
encoded biases on sensitive attributes.
However, existing studies show that adversarial learning may
not fully remove sensitive attribute biases especially when the
selection of candidate news has already been biased [23]. We notice
that if bias information does not exist in the user model, the class
distribution of the predicted attribute labels ẑ and z̃ should be both
uniform (we assume classes are balanced), which means that ẑ and
z̃ should have the same distributions. Thus, we apply an additional
KL divergence loss L𝐷 to ẑ and z̃ to regularize them to be similar,
which is formulated as follows:
L𝐷 = 𝐾𝐿(ẑ, z̃).

(3)

By optimizing this loss function, the bias distribution encoded in
candidate selection is encouraged to be similar to uniform distribution, which means that less bias information can be encoded by the
candidate-aware user model.

2.3

Model Training

We finally introduce the learning of FairRank. For the main news
ranking task, following [18] we use negative sampling strategies to
build training samples and use the InfoNCE [11] loss for optimization. The unified loss L for model training is as follows:
L = L𝑅 − 𝜆L𝐴 + L𝐷 ,

(4)

where 𝜆 is a coefficient to control the relative importance of adversarial gradients.

3 EXPERIMENTS
3.1 Datasets and Experimental Settings
We use two real-world datasets for experiments. The first one is
the dataset used in [23] (denoted as FairNews). There are 2,484
male users and 1,744 female users with observed gender labels. The
second dataset is constructed by ourselves by collecting users’ click

FairRank: Fairness-aware Single-tower Ranking Framework
for News Recommendation
logs between 06/23/2019 and 07/20/2019 on a commercial news
feeds platform (denoted as FairFeeds). There are 32,124 male users
and 17,876 female users with gender labels. The logs in the first 3
weeks are for training and validation (9:1 split), and the rest are for
test. The statistics of both datasets are listed in Table 1.
Table 1: Statistics of two news datasets.

FairNews
FairFeeds

#News
42,255
112,052

#Users
10,000
337,286

#Impressions
360,428
500,000

#Clicks
503,698
853,291

In our experiments we use Adam [7] as the optimizer, and the
learning rate is 1e-4. The adversarial loss weight 𝜆 is 0.5. The batch
size is 32. We search hyperparameters according to validation results. Motivated by [23], we choose AUC and nDCG@10 to evaluate
ranking accuracy on impression data, and use the accuracy of gender prediction from the top 10 and 20 ranked news within 100
randomly selected candidate news as fairness metrics. These metrics are indications of casual fairness [29]. We report the average
results of 5 independent experiments.

3.2

Fairness Comparison between Single-tower
and Two-tower Models

We first compare the recommendation fairness of different singletower and two-tower methods. The compared single-tower models
include: (1) DKN [16], a news recommendation method based on
candidate-aware attention; (2) DAN [32], user modeling candidateaware attention and attentional LSTM; (3) FIM [15], a fine-grained
interest matching approach for news recommendation; (4) UNBERT [31], a BERT-based news recommendation method that concatenates candidate news and clicked news into a single document.
The compared two-tower models include: (1) EBNR [10], embeddingbased news recommendation with GRU networks; (2)NAML [17], attentive multi-view learning for news recommendation; (3) NPA [18],
news recommendation with personalized attention; (4) NRMS [19],
multi-head self-attention networks for news recommendation; (5)
PLM-NR [21], pretrained language model empowered news modeling for news recommendation. The results are shown in Table 2. We
find that most single-tower ranking models generate unfairer recommendation results than two-tower models. This may be because
single-tower models conduct finer-grained matching between user
behaviors and the candidate [27], and thereby has a higher risks
of finding shortcuts and encoding bias information. It shows that
single-tower ranking models are suffered more from the biases encoded in user behavior data, and learning fair single-tower ranking
models is more challenging.

3.3

Conference’17, July 2017, Washington, DC, USA
Table 2: Fairness comparison between different single-tower
and two-tower models.

Methods
DKN
DAN
FIM
UNBERT
EBNR
NAML
NPA
NRMS
PLM-NR

FairNews
Acc@10 Acc@20
56.72
57.87
57.50
58.38
58.32
59.64
58.98
60.16
54.85
56.10
56.22
56.83
56.42
57.01
56.68
57.54
56.98
57.84

FairFeeds
Acc@10 Acc@20
57.69
58.53
58.20
59.11
59.33
60.17
59.92
60.84
55.24
56.33
56.84
57.40
57.11
57.82
57.22
57.96
57.59
58.20

Table 3: Ranking fairness of different methods. Lower scores
indicate better fairness.
Methods
DKN
DKN+AL
DKN+FairRec
DKN+FairRank
DAN
DAN+AL
DAN+FairRec
DAN+FairRank
FIM
FIM+AL
FIM+FairRec
FIM+FairRank
UNBERT
UNBERT+AL
UNBERT+FairRec
UNBERT+FairRank

FairNews
Acc@10 Acc@20
56.72
57.87
55.46
56.50
54.95
56.12
52.42
53.50
57.50
58.38
55.92
57.09
55.00
55.39
52.62
53.66
58.32
59.64
56.50
57.72
55.65
56.33
53.06
53.78
58.98
60.16
57.47
58.81
56.21
56.64
53.70
54.24

FairFeeds
Acc@10 Acc@20
57.69
58.53
56.45
57.20
56.13
56.88
52.92
53.70
58.20
59.11
56.46
57.32
55.81
56.43
52.95
53.68
59.33
60.17
57.25
57.98
55.94
56.69
53.29
53.88
59.92
60.84
57.92
58.80
56.87
57.44
53.96
54.55

and accuracy of different methods based on different basic models
are shown in Tables 3 and 4, respectively. From the results, we find
all basic single-tower models are heavily affected by the biases related to sensitive attributes, and their ranking results are unfair. In
addition, both AL and FairRec cannot effectively reduce the biases
encoded in ranking results. This is because the selection of candidate news within an impression can be biased, and it is difficult
for the single-tower models to remove their biases. By contrast,
our FairRank approach can achieve much better fairness and even
slightly better accuracy than AL and FairRec, showing that FairRank
is more appropriate for learning fair news ranking models.

Performance Comparison

Next, we verify whether FairRank can effectively mitigate the heavy
unfairness issue of single-tower news ranking framework. We compare FairRank with two fairness-aware methods, i.e., (1) AL [2],
applying adversarial learning to the candidate-aware user embedding; (2) FairRec [23], applying the decomposed adversarial learning
method to the candidate-aware user model. The ranking fairness

3.4

Ablation Study

We then study the influence of applying adversarial learning to the
candidate-invariant user embedding and the additional KL regularization loss by removing them from FairRank to compare the
accuracy and fairness changes. We use FIM as the basic model, and
the results on FairNews are shown in Fig. 2. We find that removing

Table 4: Ranking accuracy of different methods. Higher
scores indicate better accuracy.

AUC

60.0

63.6

58.0

63.2

56.0

62.8

54.0

62.4
62.0

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8

52.0
50.0

(a) FairNews.
67.5

60.0

67.2

58.0

66.9

56.0

66.6

54.0

66.3
66.0
63.0

AUC
Acc@10

58.0

AUC
Acc@10

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8

Acc@10

DKN
DKN+AL
DKN+FairRec
DKN+FairRank
DAN
DAN+AL
DAN+FairRec
DAN+FairRank
FIM
FIM+AL
FIM+FairRec
FIM+FairRank
UNBERT
UNBERT+AL
UNBERT+FairRec
UNBERT+FairRank

FairFeeds
AUC nDCG@10
65.24
32.10
64.76
31.88
64.65
31.82
64.69
31.86
66.12
32.56
65.59
32.29
65.48
32.23
65.66
32.34
67.15
33.18
66.52
32.84
66.60
32.88
66.70
32.94
68.33
33.96
67.67
33.60
67.70
33.64
67.82
33.72

AUC

Methods

FairNews
AUC nDCG@10
60.36
38.83
59.97
38.48
59.83
38.39
59.91
38.44
61.40
39.78
60.82
39.25
60.84
39.26
60.92
39.31
63.23
41.48
62.57
40.93
62.64
40.98
62.73
41.06
64.40
42.45
63.87
41.90
63.81
41.85
63.96
41.98

64.0

Acc@10

Chuhan Wu1 , Fangzhao Wu2 , Tao Qi1 , Yongfeng Huang1

Conference’17, July 2017, Washington, DC, USA

52.0
50.0

56.50

56.0

62.73

AUC

62.64

54.21

62.57

62.6

54.0

53.06

62.4

52.0
FairRank
- Candidate-invariant Embedding
- KL Loss

62.2
62.0

AUC

Acc@10

(b) FairFeeds.
Acc@10

62.8

50.0

labels inferred from both candidate-aware and candidate-invariant
user embeddings to be uniform.

48.0

3.5

(a) FairNews.
67.0

66.6

58.0

57.25

56.0

66.70

54.54
66.59
66.52

53.29

66.4
66.2
66.0

54.0
52.0

FairRank
- Candidate-invariant Embedding
- KL Loss

AUC

Acc@10

Acc@10

AUC

66.8

Figure 3: Influence of the coefficient 𝜆.

50.0
48.0

(b) FairFeeds.

Figure 2: Effectiveness of the candidate-invariant user embedding and KL loss.
bias information from the candidate-invariant user embedding is
important for improving fairness, since it is free from the interference of biases encoded by impression data and can better remove
the intrinsic biases captured by the user model. In addition, the KL
loss can also improve ranking fairness. This may be because it can
help the model to encode less bias information to make the attribute

Hyperparameter Analysis

Finally, we discuss the impact of adversarial loss weight 𝜆 on accuracy and fairness. The results on FairNews are shown in Fig. 3.
When 𝜆 is larger, the accuracy usually decreases because the adversarial loss will influence the main recommendation task, while the
fairness score does not have notable improvements when 𝜆 is larger
than 0.5. This is because the main recommendation task cannot
gain sufficient supervision signals when the adversarial loss is too
strong. Thus, we choose 𝜆 = 0.5 to achieve good fairness without
hurting too much ranking accuracy.

4

CONCLUSION

In this paper, we propose a fairness-aware single-tower ranking
framework for news recommendation, named FairRank. We propose to use a candidate-aware user model to match clicked news
with both candidate news and random news to learn a candidateaware user embedding and a candidate-invariant user embedding,
respectively, to help reduce the bias introduced by candidate news
selection. We apply adversarial learning to them to remove bias
information, and further add a KL loss that regularizes the attribute
labels inferred from the two user embeddings to be similar to make
the user model less bias-sensitive. Experiments on two datasets
show that FairRank can effectively improve the fairness of many
single-tower news ranking models with minor accuracy losses.

FairRank: Fairness-aware Single-tower Ranking Framework
for News Recommendation

REFERENCES
[1] Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe
Zhao, Lichan Hong, Ed H Chi, et al. 2019. Fairness in recommendation ranking
through pairwise comparisons. In KDD. 2212–2220.
[2] Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. 2017. Data decisions and
theoretical implications when adversarially learning fair representations. arXiv
preprint arXiv:1707.00075 (2017).
[3] Yanai Elazar and Yoav Goldberg. 2018. Adversarial Removal of Demographic
Attributes from Text Data. In EMNLP. 11–21.
[4] Suyu Ge, Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2020. Graph
Enhanced Representation Learning for News Recommendation. In WWW. 2863–
2869.
[5] Jian Hu, Hua-Jun Zeng, Hua Li, Cheng Niu, and Zheng Chen. 2007. Demographic
prediction based on user’s browsing behavior. In WWW. 151–160.
[6] Qinglin Jia, Jingjie Li, Qi Zhang, Xiuqiang He, and Jieming Zhu. 2021. RMBERT:
News Recommendation via Recurrent Reasoning Memory Network over BERT.
In SIGIR. 1773–1777.
[7] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR.
[8] Dongho Lee, Byungkook Oh, Seungmin Seo, and Kyong-Ho Lee. 2020. News
recommendation with topic-enriched knowledge graphs. In CIKM. 695–704.
[9] Danyang Liu, Jianxun Lian, Shiyin Wang, Ying Qiao, Jiun-Hung Chen,
Guangzhong Sun, and Xing Xie. 2020. KRED: Knowledge-aware document
representation for news recommendations. In Recsys. 200–209.
[10] Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017.
Embedding-based news recommendation for millions of users. In KDD. 1933–
1942.
[11] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[12] Tao Qi, Fangzhao Wu, Chuhan Wu, and Yongfeng Huang. 2021. Personalized
News Recommendation with Knowledge-aware Interactive Matching. In SIGIR.
61–70.
[13] Tao Qi, Fangzhao Wu, Chuhan Wu, Peiru Yang, Yang Yu, Xing Xie, and Yongfeng
Huang. 2021. HieRec: Hierarchical User Interest Modeling for Personalized News
Recommendation. In ACL. 5446–5456.
[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS. 5998–6008.
[15] Heyuan Wang, Fangzhao Wu, Zheng Liu, and Xing Xie. 2020. Fine-grained
Interest Matching for Neural News Recommendation. In ACL. 836–845.
[16] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep
Knowledge-Aware Network for News Recommendation. In WWW. 1835–1844.

Conference’17, July 2017, Washington, DC, USA
[17] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang,
and Xing Xie. 2019. Neural News Recommendation with Attentive Multi-View
Learning. In IJCAI. 3863–3869.
[18] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and
Xing Xie. 2019. Npa: Neural news recommendation with personalized attention.
In KDD. 2576–2584.
[19] Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie.
2019. Neural News Recommendation with Multi-Head Self-Attention. In EMNLPIJCNLP. 6390–6395.
[20] Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. 2021. Personalized News
Recommendation: A Survey. arXiv preprint arXiv:2106.08934 (2021).
[21] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering
News Recommendation with Pre-trained Language Models. In SIGIR. 1652–1656.
[22] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Two Birds
with One Stone: Unified Model Learning for Both Recall and Ranking in News
Recommendation. arXiv preprint arXiv:2104.07404 (2021).
[23] Chuhan Wu, Fangzhao Wu, Xiting Wang, Yongfeng Huang, and Xing Xie. 2021.
FairRec:Fairness-aware News Recommendation with Decomposed Adversarial
Learning. In AAAI. 4462–4469.
[24] Chuhan Wu, Fangzhao Wu, Yang Yu, Tao Qi, Yongfeng Huang, and Qi Liu.
2021. NewsBERT: Distilling Pre-trained Language Model for Intelligent News
Application. In EMNLP Findings. 3285–3295.
[25] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian,
Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. 2020. MIND: A Largescale Dataset for News Recommendation. In ACL. 3597–3606.
[26] Depeng Xu, Yongkai Wu, Shuhan Yuan, Lu Zhang, and Xintao Wu. 2019. Achieving causal fairness through generative adversarial networks. In IJCAI.
[27] Puxuan Yu, Hongliang Fei, and Ping Li. 2021. Cross-lingual Language Model
Pretraining for Retrieval. In WWW. 1029–1039.
[28] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. 2018. Mitigating unwanted biases with adversarial learning. In AIES. 335–340.
[29] Junzhe Zhang and Elias Bareinboim. 2018. Fairness in decision-making—the
causal explanation formula. In AAAI, Vol. 32.
[30] Qi Zhang, Qinglin Jia, Chuyuan Wang, Jingjie Li, Zhaowei Wang, and Xiuqiang
He. 2021. AMM: Attentive Multi-field Matching for News Recommendation. In
SIGIR. 1588–1592.
[31] Qi Zhang, Jingjie Li, Qinglin Jia, Chuyuan Wang, Jieming Zhu, Zhaowei Wang,
and Xiuqiang He. 2021. UNBERT: User-News Matching BERT for News Recommendation. In IJCAI. 3356–3362.
[32] Qiannan Zhu, Xiaofei Zhou, Zeliang Song, Jianlong Tan, and Li Guo. 2019. Dan:
Deep attention neural network for news recommendation. In AAAI, Vol. 33.
5973–5980.

