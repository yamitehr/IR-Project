See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/341478616



Neutral Bots Reveal Political Bias on Social Media

Preprint · May 2020



CITATIONS                                                                                                 READS
0                                                                                                         1,041


4 authors, including:

            Diogo Pacheco                                                                                            Kai-Cheng Yang
            University of Exeter                                                                                     Indiana University Bloomington
            36 PUBLICATIONS 210 CITATIONS                                                                            54 PUBLICATIONS 1,619 CITATIONS

              SEE PROFILE                                                                                                 SEE PROFILE



            Filippo Menczer
            Indiana University Bloomington
            270 PUBLICATIONS 22,479 CITATIONS

              SEE PROFILE




Some of the authors of this publication are also working on these related projects:


             OSoMe: The IUNI Observatory on Social Media View project



             Social Bots View project




 All content following this page was uploaded by Diogo Pacheco on 17 June 2020.

 The user has requested enhancement of the downloaded file.
                                                     Neutral Bots Reveal Political Bias
                                                              on Social Media
arXiv:2005.08141v2 [cs.SI] 27 May 2020




                                          Wen Chen, Diogo Pacheco, Kai-Cheng Yang, and Filippo Menczer

                                                                 Observatory on Social Media
                                                            Indiana University, Bloomington, USA


                                                                              Abstract
                                                  Social media platforms attempting to curb abuse and misinformation
                                               have been accused of political bias. We deploy neutral social bots on
                                               Twitter to probe biases that may emerge from interactions between users,
                                               platform mechanisms, and manipulation by inauthentic actors. We find
                                               evidence of bias affecting the news and information to which U.S. Twitter
                                               users are likely to be exposed, depending on their own political align-
                                               ment. Partisan accounts, especially conservative ones, tend to receive
                                               more followers, follow more automated accounts, and find themselves in
                                               echo chambers. Conservative accounts are exposed to more low-credibility
                                               content. Liberal accounts are exposed to moderate content shifting their
                                               experience toward the political center, while the interactions of conser-
                                               vative accounts are skewed toward the right. We find some evidence of
                                               central bias in the news feed ranking algorithm for partisan accounts.
                                               These findings help inform the public debate about how social media
                                               shape exposure to political information.


                                         Introduction
                                         Compared with traditional media, online social media can connect more people
                                         in a cheaper and faster way than ever before. As a large portion of the pop-
                                         ulation frequently use social media to generate content, consume information,
                                         and interact with others (1), online platforms are also shaping the norms and
                                         behaviors of their users. Experiments show that simply altering the messages
                                         appearing on social feeds can affect the online expressions and real-world ac-
                                         tions of users (2, 3), and that social media users are sensitive to early social
                                         influence (4, 5). At the same time, discussions on social media tend to be po-
                                         larized around critical yet controversial topics like elections (6), vaccination (7),
                                         and climate change (8). Polarization is often accompanied by the segregation
                                         of users with incongruent points of view into so-called echo chambers (9, 10),
                                         which have been associated with ideology radicalization and misinformation
                                         spreading (11, 12).


                                                                                  1
    Countering such undesirable phenomena requires a deep understanding of
their underlying mechanisms. On the one hand, several socio-cognitive biases
of humans, including selection of belief-consistent information (13) and the ten-
dency to seek homophily in social ties (14), have been identified as major con-
tributors (15, 16). On the other hand, web platforms have their own algorithmic
biases (17, 18). For example, engagement bias in ranking algorithms may cre-
ate a vicious cycle amplifying noise over quality (19, 20). For a more extreme
illustration, recent studies and media reports suggest that the YouTube recom-
mendation system might lead to videos with more misinformation or extreme
viewpoints regardless of the starting point (21).
    Beyond the socio-cognitive biases of individual users and algorithmic biases
of technology platforms, we have a very limited understanding of how collective
interactions mediated by social media may bias the view of the world that we
obtain through the online information ecosystem. The major obstacle is the
complexity of the system — not only do users exchange huge amounts of infor-
mation with large numbers of others via many hidden mechanisms, but these
interactions can be manipulated overtly and covertly by legitimate influencers
as well as inauthentic, adversarial actors who are motivated to influence opin-
ions or radicalize behaviors (22). Evidence suggests that malicious entities like
social bots and trolls have already been deployed to spread misinformation and
influence public opinion on critical matters (23–25).
    In this study, we aim to reveal biases in the news and information to which
people are exposed in social media ecosystems. We are particularly interested
in clarifying the role of social media during the polarization process and the
formation of echo chambers. We therefore focus on U.S. political discourse on
Twitter since this platform plays an important role in American politics and
strong polarization and echo chambers have been observed (6).
    Our goal of studying ecosystem bias requires the exclusion of biases from
individual users, which is a challenge when using observational methods. Social
media accounts that mimic human users but are completely controlled by algo-
rithms, known as social bots, can be used for this purpose (26). Here we deploy
social bots with unbiased random behavior as instruments to probe exposure
biases in social media. We call our bots drifters to distinguish their neutral
behavior from other types of benign and malicious social bots on Twitter (27).
    Drifters are designed with an identical behavior model but with the only
distinctive difference of their initial friend — the very first account they fol-
low. After this initial action that represents the single independent variable
in our experiment, each drifter was let loose in the wild. After five months,
we examined the content consumed and generated by the drifters and analyzed
their exposure to low-credibility information and the characteristics of their
friends and followers, including their political alignment and automated activ-
ities. This methodology allows us to examine the combined biases that stem
both from Twitter’s system design and recommendation/ranking algorithms,
and from the organic and inorganic social interactions between the drifters and
other accounts.
    We find that the political alignment of the initial friend has a major impact


                                       2
                               bot1      bot4   bot7       bot10   bot13
                               bot2      bot5   bot8       bot11   bot14
                         200   bot3      bot6   bot9       bot12   bot15




           # Followers
                         150
                         100
                         50
                          0
                                      Left
                         200          C.Left
                                      Center

           # Followers
                         150          C.Right
                         100          Right

                         50
                          0 23-Jul       22-Aug 21-Sep 21-Oct 20-Nov
Figure 1: Average numbers of followers of different drifter groups over time.
In this and other line charts, colored confidence intervals indicate ±1 standard
error.


on the popularity, social network structure, exposure to bots and low-credibility
sources, and political alignment manifested in the actions of each drifter. The
unique insights provided by our study into the political currents of Twitter’s in-
formation ecosystem can aid the public debate about how social media platforms
shape people’s exposure to political information.


Results
We developed 15 drifter bots with the same behavior model (see Methods for
details), divided them into five groups, and initialized each drifter in the same
group with the same initial friend. Each Twitter account used as a first friend
is a news source aligned with the left, center-left, center, center-right, or right
of the U.S. political spectrum (see details in Methods). We refer to the drifters
by the political alignment of their initial friends; for example, bots initialized
with center-left sources are called “C. Left” drifters.
    Between their deployment on July 10, 2019 and until their deactivation on
December 1, 2019, we monitored the behaviors of the drifters and collected data
on a daily basis. In particular, we measured: (1) the number of followers of each
drifter to compare their ability to gain influence; (2) the bot scores of friends and
followers of the drifters to check for automated activities; (3) the transitivity of
the ego network of each drifter as a proxy for echo-chamber exposure; (4) the
proportion of low-credibility information to which the drifters are exposed; and
(5) the political valence of content generated by the drifters and their friends to
probe political biases.

Influence
The number of followers can be used as a crude proxy for influence (28). To
gauge how political alignment affects influence dynamics, Fig. 1 plots the average


                                                       3
                                                          Friends
                                                          Followers
                       0.4
           Bot score
                       0.2

                       0.0 Left C.Left Center C.Right Right

Figure 2: Average bot scores of friends and followers of drifters in different
groups. The bot score is a number between zero and one, with higher scores
signaling likely automation. In this and other bar charts, error bars indicate
standard errors.


number of followers of drifters in different groups over time. Two trends emerge.
First, among drifters on the same side of the political spectrum, those with more
extreme sources as initial friends tend to attract more followers; center drifters
tend to be the least influential. Second, drifters with right-leaning initial sources
gain followers at a significantly higher rate than those with left-leaning initial
sources.

Automated Activities
Social bots were actively involved in online discussions about recent U.S. elec-
tions (23, 29, 30). It is therefore expected for the drifters to encounter automated
accounts. We used the Botometer service (31) to collect bot scores of friends
and followers of the drifters. We report the average bot scores for both friends
and followers of the drifters in Fig. 2. Focusing on the friends, we find that
accounts followed by centrist, moderate, and partisan drifters are increasingly
more bot-like, respectively. Among partisan groups, right-leaning drifters tend
to follow significantly more bots than left-leaning ones.
    Followers are more bot-like than friends for all groups, without significant
differences across the political spectrum. This is not surprising; human users
are more likely to identify the true nature of the drifters and therefore less likely
to follow them.




                                         4
Echo Chambers
We wish to investigate whether the structure of the social networks in which the
drifter bots find themselves amplifies exposure to homogeneous content. Density
and transitivity of ego networks can be used as proxies for the presence of echo
chambers, which we define as highly clustered social media neighborhoods in
which users are likely to be exposed to the same information from multiple
sources. Transitivity measures the fraction of possible triangles that are actually
present among the nodes of a network. High transitivity means that friends and
followers are likely to follow each other too. See Methods for further details.
    Fig. 3(A,B) shows the average density and transitivity of ego networks for
the drifters (see details in Methods). Since the two metrics are correlated in
an ego network, Fig. 3(C) also plots the transitivity rescaled by that of shuffled
random networks (see Methods). All metrics indicate that partisan accounts are
more densely clustered than centrists, and right-leaning accounts are in stronger
echo chambers than left-leaning ones.
    To get a better sense of what these echo-chambers look like, Fig. 3(D) maps
the ego networks of the 15 drifters. In addition to the clustered structure, we
observe a degree of homogeneity in shared content as illustrated by the colors
of the nodes, which represent the political alignment of the links shared by the
corresponding accounts (see Methods; similar results are obtained by measuring
political alignment based on shared hashtags). In general, the neighbors of a
drifter tend to share links to sources that are politically aligned with the drifter’s
first friend. We note a few exceptions, however. The left drifters and their
neighbors are more moderate, having shifted their alignment toward the center.
One of the center-left drifters has become connected to many conservative ac-
counts, shifting its alignment to the right. And one of the center-right drifters
has shifted its alignment to the left, becoming connected to mostly liberal ac-
counts after randomly following @CNN. In most cases, drifters find themselves in
structural echo chambers where they are exposed to content with homogeneous
political alignment that mirrors their own.

Exposure to Low-credibility Content
Since the 2016 U.S. presidential election, concern has been heightened about
the spread of misinformation in social media (32). We therefore analyze ex-
posure to content from low-credibility sources for different groups of drifters
in Fig. 4. Details about low-credibility sources are found in Methods. We
observe that drifters initialized with right-leaning sources receive significantly
more low-credibility content in their social feeds than other groups. For right
drifters, almost 15% of the links that appear in their timelines are from low-
credibility sources. We also measured the absolute number of low-credibility
links, and used the total number of tweets or the number of tweets with links
as the denominator of the proportions; the same pattern emerges in all cases.
    Note that Breitbart News appears in lists of hyper-partisan sources used in
the literature. However, we used @BreitbartNews as the right-leaning initial


                                          5
        A




        B




        C

  D




Figure 3: (A) Density, (B) transitivity, and (C) normalized transitivity of
drifters ego networks in different groups. (D) Ego networks of the drifters in
the five groups. Nodes represent accounts and edges represent friend/follower
relations. Node size and color represent degree (number of neighbors) and polit-
ical alignment of shared content, respectively. Black nodes have missing valence
score due to not sharing content with political valence.




                                       6
                                         15


             Low credibility links (%)
                                         10

                                         5

                                         0    Left   C.Left Center C.Right Right

    Figure 4: Proportions of low-credibility links in drifter home timelines.


friend account for right drifters because it is one of the most popular conservative
news sources. To prevent biasing our results, Breitbart News is not labeled as a
low-credibility source in this analysis and does not contribute to the proportions
in Fig. 4.

Political Alignment and Algorithmic Bias
We wish to measure the political valence of content consumed and produced by
drifters. Given a link (URL), we can extract the source (website) domain name
and obtain a valence score based on its known political slant. Similarly, given a
hashtag, we can calculate a score based on its co-occurrence with other hasthags
on Twitter that are known to signal a political alignment. These scores can then
be averaged across the links or hashtags contained in a feed of tweets to measure
their aggregate political valence. Further details can be found in Methods.
    The home timeline (also known as news feed) is the set of tweets to which
accounts are exposed. The user timeline is the set of tweets produced by an
account. In Fig. 5(A,B,D,E) we observe how the political valence of informa-
tion to which drifters are exposed in their home timelines (sh ) and of content
generated by them in their user timelines (su ) changed during the experiment.
The initial friends strongly affect the political trajectories of the drifters. Both
in terms of information to which they are exposed and content they produce,
drifters initialized with right-leaning sources stay on the conservative side of
the political spectrum. Those initialized with left-leaning sources, on the other
hand, tend to drift toward the political center: they are exposed to more con-
servative content and even start spreading it. These findings are robust with
respect to the method used to calculate political valence, whether based on
hashtags (Fig. 5(A,B)) or links (Fig. 5(D,E)).
    We measure the political bias of the home timeline ranking algorithm by
calculating the difference between the valence score of tweets posted by friends of


                                                           7
    Dec-2019 A                    B                 C
    Nov-2019
    Oct-2019                                                          Left
    Sep-2019                                                          C.Left
                                                                      Center
    Aug-2019                                                          C.Right
                                                                      Right
             -0.5 0 0.5         -0.5 0      0.5 -0.5 0        0.5
    Dec-2019 D                     E               F
    Nov-2019
    Oct-2019
    Sep-2019
    Aug-2019
                -0.5 0 0.5        -0.5 0 0.5      -0.5 0 0.5
                     sh                su             sh sf
Figure 5: Time series of political valence scores calculated from hashtags (top)
and links (bottom). Negative scores mean left-leaning and positive scores mean
right-leaning. Valence scores sh to which drifter are exposed in their home
timelines, based on (A) hashtags and (D) links. Valence scores su expressed
by drifter posts in their user timelines, based on (B) hashtags and (E) links.
Algorithmic bias sh −sf experienced by drifters, where the political valence score
sf is derived from the content generated by friends, based on (C) hashtags and
(F) links. Missing values are replaced by preceding available ones. Plots for
each individual drifter are in Supplementary Materials.




                                        8
the drifters, sf , and the score of tweets in the home timeline, sh . The algorithm
ranks the latter among the former. The results are shown in Fig. 5(C,F) for
valence computed from hashtags and links, respectively. In the case of hashtags,
we observe little evidence of political bias by the algorithm. For right-leaning
drifters, there is a small but significant shift to the left, suggesting a weak bias
of the platform algorithm (Fig. 5(C)). To confirm this visual observation, we
performed a paired t-test comparing the daily averages of home timeline scores
sh and friend user timeline scores sf . The effect is small or insignificant for all
but the right group of drifters, for which the effect size is medium (t = −10.8,
p < 0.01, Cohen’s d = 0.58). In the case of links (Fig. 5(F)), we observe evidence
of significant center-tendency bias for drifters in the left group (t = +19.6,
p < 0.01, Cohen’s d = 0.99) and right group (t = −17.5, p < 0.01, Cohen’s
d = 0.94). For the other groups, the effect is small or insignificant. Further
details on the bias analysis can be found in Supplementary Materials.


Discussion
Social bots can be used as unbiased instruments to probe political (and other)
biases in online information ecosystems. Though we examined Twitter in this
paper, the same methodology (and our software) can be applied by the research
community in different contexts. It will be interesting to see if our findings can
be replicated on other platforms, other countries, with more drifter bots, and
during elections.
    The present results suggest that early choices about which sources to follow
have a strong impact on the experiences of social media users. This is consistent
with previous studies (4, 5). But beyond those initial actions, drifter bots are
designed to be neutral with respect to partisan content and users. Therefore
the partisan-dependent differences in their experiences and behaviors can be
attributed to their interactions with users and information mediated by the
social media platform — they reflect biases of the online information ecosystem.
    Drifters with right-wing initial friends are gradually embedded into dense
and homogeneous networks where they are constantly exposed to right-leaning
content. They even start to spread right-leaning content themselves. Such
online feedback loops reinforcing group identity may lead to radicalization (11),
especially in conjunction with social and cognitive biases like in-/out-group bias
and group polarization.
    The fact that right-leaning drifters are exposed to considerably more low-
credibility content than other groups is in line with previous findings that con-
servative users are more likely to engage with misinformation on social me-
dia (33). Our experiment suggests that the ecosystem can lead completely
unbiased agents to this condition, therefore it is not necessary to impute the vul-
nerability to individual characteristics. Other mechanisms that may contribute
to the exposure to low-credibility content observed for the drifters initialized
with right-leaning sources involve the actions of neighbor accounts (friends and
followers) in the right-leaning groups, including inauthentic accounts that target


                                         9
these groups.
     While most drifters are parts of clustered and homogeneous network com-
munities, the echo chambers of conservative accounts grow especially dense and
include a larger portion of politically active accounts. Social bots also seem to
play an important role in the partisan social networks; the drifters, especially
right-leaning ones, end up following a lot of them. Since bots also amplify the
spread of low-credibility news (23), this may help explain the prevalent expo-
sure of right-leaning drifters to low-credibility sources. Drifters initialized with
far-left sources do gain more followers, follow more bots, and form denser so-
cial networks compared with the center and center-left groups. However this
occurs in a way that is less emphatic and vulnerable to low-credibility content
compared to the right and center-right groups.
     Twitter has been accused of favoring liberal content and users. Our analysis
of temporal shifts in political valence of the drifters and their friends reveals
a complex picture in which the ecosystem and the platform have different bi-
ases. We examined the possible bias in Twitter’s news feed ranking algorithm,
i.e., whether the content to which a user is exposed in the home timeline is se-
lected in a way that amplifies or suppresses certain political content produced by
friends. Our results suggest this is not the case in general: the drifters seem to
receive content that is closely aligned with whatever their friends produce. Ex-
ceptions are observed for extreme partisan drifters, depending on the method
used to calculate political valence. While hashtag analysis uncovers a weak
central-tendency bias only among conservative accounts, link analysis suggests
the presence of a stronger central-tendency bias for both liberal and conservative
drifters. Such an algorithmic bias may not be due to any intentional interfer-
ence by the platform. Other explanations are possible; for example, Twitter
may remove or demote information from low-credibility sources and/or from
inauthentic accounts. To the extent that such content tends to be partisan, the
net result would be a bias toward the center. Measures of political valence based
on links are more likely to detect shifts of this kind.
     On the other hand, drifters that start with left-leaning sources shift toward
the right during the course of the experiment, sharing and being exposed to
more moderate content. Drifters that start with right-leaning sources do not
experience a similar exposure to moderate information and produce increasingly
partisan content. These results are consistent with observations that right-
leaning bots do a better job at influencing users (34). In summary, we observe
a net conservative bias that emerges from the complex interactions within the
information ecosystem.
     Our experiment demonstrates that even if a platform has no partisan bias in
its algorithms and policies, the social networks and activities of its users may still
create an environment in which unbiased agents end up in echo chambers with
constant exposure to partisan, inauthentic, and misleading content. Users have
to make extra efforts to moderate the content they consume and the social ties
they form in order to counter these currents and create a healthy and balanced
online experience. Platform policies must acknowledge that neutral algorithms
do not necessarily yield neutral outcomes. A key question is how to design


                                         10
mechanisms capable of mitigating the biases that emerge in online information
ecosystems.


Methods
Here we provide details about the design of Drifter bots, the computation of
political valence metrics, the identification of low-credibility sources, and the
characterization of echo chambers. All of our code and data used to run the ex-
periment and produce the figures in this paper is available in a public repository
(github.com/IUNetSci/DrifterBot).

Drifter Behavior Model
Drifter bots are the key instrument for this study. They are designed to mimic
social media users, so that the data collected from their actions and interactions
reflects realistic experiences on the platform. The drifters lack any ability to
comprehend the content to which they are exposed or the users with whom they
interact. All actions are controlled by a stochastic model which was unchanged
during the experiment.
    Like many human behaviors, social media activity is bursty (35). To re-
produce this feature, we draw time intervals ∆t between two successive actions
from a power-law distribution P (∆t) ∼ ∆t−α , with exponent α = 0.9 manually
tuned to minimize the bot score obtained from the Botometer service. The
distribution was cutoff at a maximum sleep duration of seven hours between
consecutive actions. Intervals were further scaled to obtain a reasonable aver-
age frequency of 20–30 actions per day. Moreover, the drifters were inactive
between midnight and 7 a.m.
    Every time a drifter is activated, it randomly selects an action and a source as
illustrated in Fig. 6. Actions include tweets, retweets, likes, replies, etc. Sources
include the home timeline, trends, friends, etc. Each action is selected with a
predefined probability. Given the selected action, one of a set of possible sources
is selected with a predefined conditional probability. A random object is then
drawn from the source and the action is performed on it. For example, if the
action is a retweet and the source is the home timeline, then a random tweet in
the drifter’s home timeline is retweeted. Non-English sources (users and tweets)
are disregarded when they can be identified from metadata. Finally, the bot
sleeps for a random interval until the next action. To avoid behaviors typical of
spam bots that violate Twitter’s polices, the follow and unfollow actions have
additional constraints regarding the ratio between friends and followers. The
constraints are mutually exclusive, so that if one of these two actions fails due
to a constraint not being satisfied, the other action in performed. Details about
actions, sources, their associated probabilities, and constraints can be found in
Supplementary Materials.
    When creating the drifter profiles, we avoided any political references that
would bias the experiment as well as deceptive impersonation that would violate


                                         11
                                           START




 Action     Reply     Tweet    Retweet          Like        Follow      Unfollow




 Source                                                        Few         No       Enough
                                                            friends?                friends?

                                                                 Yes
                                                                                          Yes



 Mention    Random                Home      Tweets liked   Friends of
                      Trends                                            Followers   Friends
 timeline    quotes             timeline     by friends     friends



Figure 6: Drifter bot behavior model. Each action box is connected with boxes
that indicate the sources used for that action. For example, the source of a
retweet can be a trending tweet, a tweet in the home timeline, or a tweet liked
by a friend. Links to actions and sources are associated with probabilities.
Follow and unfollow actions require additional constraints to be satisfied (gray
diamonds).


platform policies. The screen names and user names were selected from famous
bots in the arts and literature. Corresponding profile images were drawn from
the public domain. Random quotes were used as profile descriptions. Since the
drifter bots interacted with human subjects, the experiment was vetted by the
Indiana University’s ethics board and deemed exempt from review.
    The only difference among the drifters was the way their friend lists were
initialized. This was our experiment’s independent variable. We started from
five Twitter accounts associated with established, active, and popular U.S. news
sources: The Nation (left), The Washington Post (center-left), USA Today (cen-
ter), The Wall Strett Journal (center-right), and Breitbart News (right). These
sources were selected as they span the full range of the U.S. political spec-
trum, according to the valence of the corresponding websites computed with a
method described below. The 15 drifters were divided into five groups so that
each of three bots in the same group started by following the same source. The
friend list of each drifter was then expanded by following a random sample of
five English-speaking friends of the first friend, and a random sample of five
English-speaking followers of the first friend — 11 accounts in total.

Political Alignment Metrics and Algorithmic Bias
Given our political bias, we need to measure the political alignment of tweets
and accounts. We adopt two independent approaches, one based on hashtags
and one on links, to ensure the robustness of our results. Both approaches start
with assigning political valence scores to entities that may be present in tweets,
namely hashtags and links. See Supplementary Materials for details about how


                                           12
these entities are extracted from tweets. The entity scores are then averaged at
the tweet level to obtain valence scores for the tweets, and further at the user
level to measure the political alignment of users.
     The hashtag-based approach relies on hashtags (keywords preceded by the
hash mark #) commonly included by users in their social media posts because
they are concise and efficient ways to label topics, ideas, or memes so that oth-
ers can find the messages. Hashtags are often used to signal political identities,
beliefs, campaigns, or alignment. We apply the word2vec algorithm (36) to as-
sign political valence scores to hashtags in a semi-supervised fashion. Word2vec
maps words in text to continuous vector representations. The axis between a
pair of carefully selected word vectors can encode a meaningful cultural dimen-
sion and an arbitrary word’s position along this axis reflects its association with
this cultural dimension. Using hashtags as words, we look for an axis repre-
senting the political alignment in the embedding vector space. We leverage a
dataset of political tweets collected during the 2018 U.S. midterm elections (37).
The hashtags from the same tweet are grouped together as a single “sentence”
and fed to the word2vec algorithm to obtain vector representations for the hash-
tags. After removing hashtags appearing less than five times in the dataset, we
end up with 54,533 hashtag vectors. To define the political alignment axis, we
choose #voteblue and #votered as two poles because they show clear align-
ment with U.S. liberal and conservative political orientations, respectively. The
rest of the hashtags are then projected onto this axis, and the relative positions,
scaled into the interval [−1, 1], are used to measure the political valence where
negative/positive scores indicate left/right alignment.
     The link-based approach considers links (URLs) commonly used to share
news and other websites in social media posts, for the purpose of spreading
information, expressing opinions, or flagging identity, especially around politi-
cal matters. Many websites show clear political bias and the number of popular
ones is quite limited. Therefore the websites (domains) extracted from links pro-
vide another convenient proxy for the political alignment of tweets and users. To
assess the political alignment of a website, we start with a dataset of 500 ideolog-
ically diverse news sources (38), where each domain is assigned a scores reflect-
ing its political valence in the liberal-conservative range [−1, +1]. We manually
clean the dataset by removing outdated domains and updating the ones with
new names. For example, myfoxdetroit.com becomes fox2detroit.com. For
each link found in the tweets, we matched the domain to the list to obtain a
score. See Supplementary Materials for additional details.
     We further aggregate the political valence scores of the tweets, obtained us-
ing either hashtags or links, at the account level. We examine different types
of political valence for accounts, each measured on a daily basis. The political
valence to which a drifter is exposed, sh , is computed by averaging the scores
of 50 most recent tweets from its home timelines. We also evaluate the po-
litical valence expressed by an accounts by averaging recent tweets they post.
We measure this expressed valence su for each drifter using its most recent 20
tweets. We use sf to represent the political valence expressed by the friends of
each drifters, using their 500 collective tweets. In the timeline plots, missing


                                        13
values are replaced by preceding available ones. In Supplementary Materials we
detail how political valence scores are calibrated so that a value of zero can be
interpreted as aligned with the political center.
    Since sf represents the political alignment expressed by the friends of a
drifter and sh represents the valence of the posts to which the drifter is actually
exposed in its home timeline, the difference sh − sf can be used to measure any
political bias in Twitter’s ranking algorithm that prioritize posts on one’s news
feed.

Identification of Low-credibility Content
In evaluating the credibility of the content to which drifters are exposed, we
focus on the sources of shared links to circumvent the challenge of assessing the
accuracy of individual news articles (32). Annotating content credibility at the
domain (website) level rather than the link level is an established approach in
the literature (23, 33, 39, 40).
    We use a list of low-credibility sources complied from several recent research
papers. Specifically, we consider a source as low-credibility if it fulfills any
one of the following criteria: (1) labeled as low-credibility by Shao et al. (23);
(2) labeled as “Black,” “Red,” or “Satire” by Grinberg et al. (33); (3) labeled
as “fake news” or “hyperpartisan” by Pennycook et al. (39); or (4) labeled as
“extreme left,” “extreme right,” or “fake news” by Bovet et al. (40). This provides
us with a list of 570 sources.
    To measure the percentage of low-credibility links, we extracted the links
from the home timelines of the drifters (expanding those that are shortened)
and then cross-referenced them with the list of low-credibility sources.

Echo Chambers
We wish to measure the density and transitivity of each drifter bot’s ego net-
work. Since reconstructing the full network of friends and followers of each bot
is prohibitively time consuming due to the Twitter API’s rate limits, we approx-
imated each bot’s ego network by sampling 100 random neighbors from a list of
the latest 200 friends and 200 followers returned by the Twitter API. We then
checked each pair of sampled neighbors for friendship. We add an undirected
edge if there is a follower/friend link in either direction, so that the sampled ego
network is undirected and unweighted. Finally, we computed the density and
transitivity of each ego network.
    Since transitivity is correlated with density, we also normalized the transi-
tivity by the average transitivity of 30 shuffled networks generated by a configu-
ration model that preserves the degree sequence of the original ego network. We
replace any self-loop and parallel edges, generated by the configuration model,
by random edges.




                                        14
Acknowledgement
We are grateful to Paul Cheung for a conversation that inspired the drifter ex-
periment. This work was supported in part by Knight Foundation and Craig
Newmark Philanthropies. Any opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the authors and do not necessarily
reflect the views of the funding agencies.


References
 [1] Andrew Perrin and Monica Anderson. Share of us adults using social media,
     including facebook, is mostly unchanged since 2018. Pew Research Center,
     10, 2019.

 [2] Adam DI Kramer, Jamie E Guillory, and Jeffrey T Hancock. Experimen-
     tal evidence of massive-scale emotional contagion through social networks.
     Proceedings of the National Academy of Sciences, 111(24):8788–8790, 2014.
 [3] Robert M Bond, Christopher J Fariss, Jason J Jones, Adam DI Kramer,
     Cameron Marlow, Jaime E Settle, and James H Fowler. A 61-million-
     person experiment in social influence and political mobilization. Nature,
     489(7415):295–298, 2012.
 [4] Lev Muchnik, Sinan Aral, and Sean J Taylor. Social influence bias: A
     randomized experiment. Science, 341(6146):647–651, 2013.
 [5] Tim Weninger, Thomas James Johnston, and Maria Glenski. Random vot-
     ing effects in social-digital spaces: A case study of reddit post submissions.
     In Proceedings of the 26th ACM conference on hypertext & social media,
     pages 293–297. ACM, 2015.
 [6] Michael D Conover, Jacob Ratkiewicz, Matthew Francisco, Bruno
     Gonçalves, Filippo Menczer, and Alessandro Flammini. Political polar-
     ization on twitter. In Fifth international AAAI conference on weblogs and
     social media, 2011.
 [7] Ana Lucía Schmidt, Fabiana Zollo, Antonio Scala, Cornelia Betsch, and
     Walter Quattrociocchi. Polarization of the vaccination debate on facebook.
     Vaccine, 36(25):3606–3612, 2018.

 [8] Hywel TP Williams, James R McMurray, Tim Kurz, and F Hugo Lambert.
     Network analysis reveals open forums and echo chambers in social media
     discussions of climate change. Global environmental change, 32:126–138,
     2015.
 [9] R Kelly Garrett. Echo chambers online?: Politically motivated selective
     exposure among internet news users. Journal of Computer-Mediated Com-
     munication, 14(2):265–285, 2009.


                                        15
[10] Jae Kook Lee, Jihyang Choi, Cheonsoo Kim, and Yonghwan Kim. Social
     media, network heterogeneity, and opinion polarization. Journal of com-
     munication, 64(4):702–722, 2014.
[11] Magdalena Wojcieszak. ‘don’t talk to me’: effects of ideologically homo-
     geneous online groups and politically dissimilar offline ties on extremism.
     New Media & Society, 12(4):637–655, 2010.
[12] Michela Del Vicario, Alessandro Bessi, Fabiana Zollo, Fabio Petroni, An-
     tonio Scala, Guido Caldarelli, H Eugene Stanley, and Walter Quattrocioc-
     chi. The spreading of misinformation online. Proceedings of the National
     Academy of Sciences, 113(3):554–559, 2016.

[13] Raymond S Nickerson. Confirmation bias: A ubiquitous phenomenon in
     many guises. Review of general psychology, 2(2):175–220, 1998.
[14] Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of
     a feather: Homophily in social networks. Annual review of sociology,
     27(1):415–444, 2001.

[15] Michela Del Vicario, Antonio Scala, Guido Caldarelli, H Eugene Stanley,
     and Walter Quattrociocchi. Modeling confirmation bias and polarization.
     Scientific reports, 7:40391, 2017.
[16] Kazutoshi Sasahara, Wen Chen, Hao Peng, Giovanni Luca Ciampaglia,
     Alessandro Flammini, and Filippo Menczer. On the inevitability of online
     echo chambers. arXiv preprint arXiv:1905.03919, 2019.
[17] Ricardo Baeza-Yates. Bias on the web. Communications of the ACM,
     61(6):54–61, 2018.
[18] Dimitar Nikolov, Mounia Lalmas, Alessandro Flammini, and Filippo
     Menczer. Quantifying biases in online information exposure. Journal of
     the Association for Information Science and Technology, 70(3):218–229,
     2019.
[19] Giovanni Luca Ciampaglia, Azadeh Nematzadeh, Filippo Menczer, and
     Alessandro Flammini. How algorithmic popularity bias hinders or promotes
     quality. Scientific reports, 8(1):1–7, 2018.
[20] Mihai Avram, Nicholas Micallef, Sameer Patil, and Filippo Menczer. Expo-
     sure to social engagement metrics increases vulnerability to misinformation.
     Preprint 2005.04682, arXiv, 2020.

[21] Manoel Horta Ribeiro, Raphael Ottoni, Robert West, Virgílio AF Almeida,
     and Wagner Meira Jr. Auditing radicalization pathways on youtube. In
     Proceedings of the 2020 Conference on Fairness, Accountability, and Trans-
     parency, pages 131–141, 2020.



                                       16
[22] Robin Thompson. Radicalization and the use of social media. Journal of
     strategic security, 4(4):167–190, 2011.
[23] Chengcheng Shao, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng
     Yang, Alessandro Flammini, and Filippo Menczer. The spread of low-
     credibility content by social bots. Nature Communications, 9(1):1–9, 2018.

[24] Massimo Stella, Emilio Ferrara, and Manlio De Domenico. Bots increase
     exposure to negative and inflammatory content in online social systems.
     PNAS, 115(49):12435–12440, 2018.
[25] David A Broniatowski, Amelia M Jamison, SiHua Qi, Lulwah AlKulaib,
     Tao Chen, Adrian Benton, Sandra C Quinn, and Mark Dredze. Weaponized
     health communication: Twitter bots and russian trolls amplify the vaccine
     debate. Am. J. of Public Health, 108(10):1378–1384, 2018.
[26] Eduardo Hargreaves, Claudio Agosti, Daniel Menasché, Giovanni Neglia,
     Alexandre Reiffers-Masson, and Eitan Altman. Fairness in online social
     network timelines: Measurements, models and mechanism design. Perfor-
     mance Evaluation, 129:15–39, 2019.
[27] Emilio Ferrara, Onur Varol, Clayton Davis, Filippo Menczer, and Alessan-
     dro Flammini. The rise of social bots. Communications of the ACM,
     59(7):96–104, 2016.

[28] Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto, and Krishna P
     Gummadi. Measuring user influence in Twitter: The million follower fal-
     lacy. In Proc. 4th Intl. AAAI Conference on Weblogs and Social Media
     (ICWSM), 2010.
[29] Alessandro Bessi and Emilio Ferrara. Social bots distort the 2016 US pres-
     idential election online discussion. First Monday, 21(11), 2016.
[30] Ashok Deb, Luca Luceri, Adam Badaway, and Emilio Ferrara. Perils and
     challenges of social media and election manipulation analysis: The 2018 us
     midterms. In Companion Proc. WWW Conf., pages 237–247, 2019.
[31] Kai-Cheng Yang, Onur Varol, Clayton A. Davis, Emilio Ferrara, Alessandro
     Flammini, and Filippo Menczer. Arming the public with artificial intelli-
     gence to counter social bots. Human Behavior and Emerging Technologies,
     1(1):48–61, 2019.
[32] David Lazer, Matthew Baum, Yochai Benkler, Adam Berinsky, Kelly
     Greenhill, Filippo Menczer, et al. The science of fake news. Science,
     359(6380):1094–1096, 2018.
[33] Nir Grinberg, Kenneth Joseph, Lisa Friedland, Briony Swire-Thompson,
     and David Lazer. Fake news on twitter during the 2016 us presidential
     election. Science, 363(6425):374–378, 2019.


                                      17
[34] Luca Luceri, Ashok Deb, Adam Badawy, and Emilio Ferrara. Red bots
     do it better: Comparative analysis of social bot partisan behavior. In
     Companion Proceedings of The 2019 World Wide Web Conference, pages
     1007–1012, 2019.
[35] Rumi Ghosh, Tawan Surachawala, and Kristina Lerman. Entropy-based
     classification of ‘retweeting’ activity on twitter. In Proc. 4th Workshop on
     Social Network Mining and Analysis (SNA-KDD), 2011.
[36] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.
     Distributed representations of words and phrases and their compositional-
     ity. In Advances in neural information processing systems, pages 3111–3119,
     2013.
[37] Kai-Cheng Yang, Pik-Mai Hui, and Filippo Menczer. Bot electioneering
     volume: Visualizing social bot activity during elections. In Companion
     Proceedings of The 2019 World Wide Web Conference, pages 214–217, 2019.
[38] Eytan Bakshy, Solomon Messing, and Lada Adamic. Replication data for:
     Exposure to ideologically diverse news and opinion on facebook. Harvard
     Dataverse, 2, 2015.
[39] Gordon Pennycook and David G Rand. Fighting misinformation on social
     media using crowdsourced judgments of news source quality. Proceedings
     of the National Academy of Sciences, 116(7):2521–2526, 2019.

[40] Alexandre Bovet and Hernán A Makse. Influence of fake news in twitter
     during the 2016 us presidential election. Nature communications, 10(1):1–
     14, 2019.




                                       18
Supplementary Materials
Supplementary methods
Drifter Actions and Probabilities
An action is performed upon a sentence, an existing tweet, or a user. These
inputs are selected from sources that are described below. A drifter can perform
the following actions:
   • Tweet – post a sentence from Random Quotes, Trends, or Home Timeline.
     For Trends, the sentence is the text of the selected tweet. For Home
     Timeline, the sentence is obtained by concatenating a short phrase from
     a manually compiled list (e.g., “Wow!” or “Maybe so.”) with the link of
     the selected tweet. This emulates a quoted tweet.
   • Retweet – select a tweet from Trends, Home Timeline, or a list of Tweets
     Liked by its Friends, and retweet it.
   • Like – like a tweet selected in the same way as for a Retweet.
   • Reply – reply to a tweet from the Mention Timeline. The reply is gen-
     erated using the ChatterBot library (chatterbot.readthedocs.io). In
     case of failure, the reply is a random phrase from the precompiled list
     described above.
   • Follow – select a user to follow from the list of Followers, Friends of
     Friends, users who posted Tweets liked by Friends, or users who posted
     tweets in the Home Timeline.
   • Unfollow – select a user to unfollow from the latest 200 in the list of
     Friends.
    Input elements for actions are selected from candidate lists that we call
sources. The selection is random with uniform probability distribution unless
otherwise explained below. Due to limitations of the Twitter APIs, we imitate
some basic mechanisms offered by the platform, such as suggestions to follow
friends of friends. Sources are defined as follows:
   • Random Quotes – sentences obtained from a random quote API (api.
     quotable.io/random).
   • Mention Timeline – the latest 10 tweets in the mention timeline. If
     the drifter replied to any mentions in the past, this source only considers
     subsequent tweets.
   • Friends of Friends – the model randomly selects three friends of the
     drifter and requests their latest 5,000 friends, ignoring those that are al-
     ready friends of the drifter. A new friend is selected from the combined
     list with probability proportional to the occurrences in the list, to favor
     friends of multiple friends.


                                       19
   • Friends – most recent 200 friends. The user is selected from this list at
     random, but older friends are more likely to be unfollowed. We implement
     this mechanism by ranking friends chronologically; the latest friend has
     rank one. The unfollow probability is proportional to the rank. The initial
     friend can never be unfollowed.

   • Followers – most recent 200 followers.
   • Trends – list obtained by randomly selecting three trending topics in the
     U.S. and fetching the top five tweets in each topic by the default ranking.
   • Tweets Liked by Friends – start from the latest 15 tweets from the
     home timeline. Select a random subset of at most ten friends who posted
     these tweets. Select the three latest tweets liked by each of the selected
     friends, excluding any by the drifter itself. Select one tweet at random
     from this combined list. Depending on the selected action, the source can
     return the tweet itself (for Retweet or Like) or its author (for Follow).

   • Home Timeline – the latest 15 tweets in the home timeline.

    We list the probabilities used in the bot behavior model in Table S1. The
numbers are inferred from a random sample of Twitter users. If the Follow
or Unfollow action is selected, a precondition check is triggered. If the Follow
precondition is not met, the Unfollow action is performed and viceversa; the
two checks cannot both fail. A new friend can only be followed if the number of
friends is sufficiently small compared to the number of followers: less than the
number of followers plus 113. A friend can only be unfollowed if the drifter has
at least 50 friends.


Table S1: Probabilities of actions and sources in the drifter bot behavior model.
The probabilities of the actions add up to one, and so do the conditional prob-
abilities of the sources given each action.
     Action       P (Action) Source                        P (Source|Action)
     Reply           0.05      Mention Timeline                   1.0
                               Random Quotes                      0.3
     Tweet           0.15      Trends                             0.3
                               Home Timeline                      0.4
                               Trends                             0.1
     Retweet         0.1
                               Home Tiemline                      0.6
     Like            0.35
                               Tweets Liked by Friends            0.3
                               Home Timeline                      0.2
                               Tweets Liked by Friends            0.2
     Follow          0.25
                               Friends of Friends                 0.5
                               Followers                          0.1
     Unfollow         0.1      Friends                            1.0



                                       20
Extraction of hashtags and links
We accessed tweets using the Twitter API. Links (URLs) and hashtags were
extracted from entities metadata. Tweets longer than 140 characters are trun-
cated; in these cases, we extracted links and hashtags from the extended_entities
metadata except for 4% of the tweets, for which this retrieval process failed.
    Many links are compressed using URL-shortening services. We expanded
shortened links via HTTP HEAD requests using a heuristics based on the length
of the URL (20 characters of less), allowing multiple redirects with a 10-second
timeout.

Calibration of Political Valence Scores
We calibrated valence scores so that positive scores mean right-leaning hash-
tags/links and negative scores mean left-leaning hashtags/links. To this end,
we selected the news source account @USATODAY to have a zero valence score.
We used the 200 most recent tweets by @USATODAY in early June to calculate the
raw center valence score sc . We obtained sc = −0.0635 and sc = −0.2456 for
the link-based and hashtag-based approach, respectively. The political valence
scores are then calibrated by:
                                     N
                                   1 X
                              s=       (ti − sc ),
                                   N i

where ti is the score for tweet i and N is the number of tweets across which the
score is aggregated.

Supplementary Analyses
In this section, we provide additional results and analyses for the political va-
lence estimations and algorithmic biases. Fig. 5 in the main text shows the
aggregated political valence scores. We further provide the individual trajec-
tory of the political alignment for each drifter. Fig. S1 shows the results from
the link-based approach and Figure S2 shows the results from the hashtag-based
approach. Finally, Fig. S3 shows the algorithmic bias computed for each drifter
with both methods. Table S2 shows the results of the analysis of algorithmic
bias in the platform’s home timeline ranking algorithm.




                                       21
                  Left       Center-left    Center Center-right        Right      Summary
     Dec-2019
     Nov-2019
A    Oct-2019
     Sep-2019
     Aug-2019
                   Left0.5
                -0.50        Center-left
                              -0.50 0.5   Center
                                               0.5 Center-right
                                         -0.50       -0.50 0.5    Right
                                                                -0.50 0.5         Summary
                                                                                   -0.50 0.5
     Dec-2019
     Nov-2019
B    Oct-2019
     Sep-2019
     Aug-2019
                -0.50 0.5     -0.50 0.5     -0.50 0.5    -0.50 0.5    -0.50 0.5   -0.50 0.5

Figure S1: Political valence timelines based on links for all fifteen bots. A tweet
is assigned a score between −1 (liberal) and +1 (conservative) based on the
shared link domains. (A) Home timeline: daily average score of the last 50
tweets in the home timeline. (B) User timeline: daily average score of the last
20 tweets in the user timeline. The summary represents the average for each
group.




                  Left       Center-left    Center Center-right        Right      Summary
     Dec-2019
     Nov-2019
A    Oct-2019
     Sep-2019
     Aug-2019
             -0.5Left
                  0 0.5      Center-left     0 0.5 Center-right
                                          Center
                             -0.5 0 0.5 -0.5        -0.5 0 0.5 -0.5Right   Summary
                                                                     0 0.5 -0.5 0 0.5
     Dec-2019
     Nov-2019
B    Oct-2019
     Sep-2019
     Aug-2019
             -0.5 0 0.5      -0.5 0 0.5    -0.5 0 0.5   -0.5 0 0.5   -0.5 0 0.5   -0.5 0 0.5

Figure S2: Political valence timelines based on hashtags for all fifteen bots. A
tweet is assigned a score between −1 (liberal) and +1 (conservative) based on
the shared hashtags. (A) Home timeline: daily average score of the last 50
tweets in the home timeline. (B) User timeline: daily average score of the last
20 tweets in the user timeline. The summary represents the average for each
group.




                                                 22
                                       Left     Center-left   Center Center-right     Right    Summary
                              Dec-2019
                              Nov-2019
                         A    Oct-2019
                              Sep-2019
                              Aug-2019
                                     -0.5 Left        0 0.5 -0.5Center
                                                 Center-left
                                           0 0.5 -0.5                   -0.5 0 0.5 -0.5Right
                                                                  0 0.5 Center-right            Summary
                                                                                         0 0.5 -0.5 0 0.5
                              Dec-2019
                              Nov-2019
                         B    Oct-2019
                              Sep-2019
                              Aug-2019
                                     -0.5 0 0.5 -0.5 0 0.5 -0.5 0 0.5 -0.5 0 0.5 -0.5 0 0.5 -0.5 0 0.5

                         Figure S3: Algorithmic bias for all fifteen bots, measured by the difference in
                         valence between the account’s home timeline and its friends’ user timelines,
                         based on (A) links and (B) hashtags. The summary represents the average for
                         each group.




                         Table S2: Results of paired t-test analysis comparing political valence scores
                         of drifter home timelines and their friends’ user timelines. Negative t values
                         indicate left (liberal) bias, positive values indicate right (conservative) bias.
                                 Drifters     Method        t    p < 0.01 Cohen’s d Effect Size
                                 Left         hashtag -6.3         True         0.32         small
                                 C. Left      hashtag      2.6     True         0.13         small
                                 Center       hashtag      1.8     False         —             —
                                 C. Right hashtag          4.5     True         0.23         small
                                 Right        hashtag -10.8        True         0.58       medium
                                 Left           link      19.6     True         0.99         large
                                 C. Left        link      -2.0     False         —             —
                                 Center         link      -6.0     True         0.31         small
                                 C. Right       link      -8.9     True         0.46         small
                                 Right          link     -17.5     True         0.94         large




                                                                  23




View publication stats
